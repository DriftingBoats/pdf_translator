#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
page_batch_translation_agent_cn.py
â€”â€” æŒ‰é¡µæ•°åˆ†æ‰¹ PDF è‹±è¯‘æœ¬ â†’ ä¸­æ–‡è¯‘æœ¬æ‰¹é‡ç¿»è¯‘è„šæœ¬
  â€¢ æŒ‰æŒ‡å®šé¡µæ•°Xè‡ªåŠ¨åˆ†æ‰¹ç¿»è¯‘
  â€¢ å¤„ç†å¥å­å®Œæ•´æ€§ï¼Œç¡®ä¿ç¿»è¯‘è‡³å¥å­ç»“æŸ
  â€¢ è‡ªåŠ¨è¯†åˆ«æ ‡é¢˜å¹¶ä¿æŒæ ¼å¼
  â€¢ è‡ªåŠ¨ç¼–å·å¹¶æœ€ç»ˆæ•´åˆä¸ºä¸€ä¸ªæ–‡ä»¶
  â€¢ ä½¿ç”¨promptçº¦æŸä¸“æœ‰åè¯å¤„ç†
"""
import json, re, textwrap, logging, time, datetime, requests, os, sys, warnings, random
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# è¿‡æ»¤PyMuPDFçš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning, module="fitz")

import fitz  # PyMuPDF - pip install PyMuPDF

# æˆæœ¬è·Ÿè¸ªå…¨å±€å˜é‡
total_cost = 0.0
total_input_tokens = 0
total_output_tokens = 0

# ========= ç¼“å­˜ç®¡ç†åŠŸèƒ½ ========= #
def clean_cache_files(cache_dir: Path, pdf_path: Path = None, force: bool = False):
    """æ¸…ç†ç¼“å­˜æ–‡ä»¶
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•
        pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
    """
    if not cache_dir.exists():
        return
    
    cache_patterns = [
        "*_text_cache.txt",      # PDFæ–‡æœ¬ç¼“å­˜
        "batch_*_raw_text.txt",  # æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    ]
    
    cleaned_count = 0
    total_size_cleaned = 0
    
    logging.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ç¼“å­˜æ–‡ä»¶ (å¼ºåˆ¶æ¸…ç†: {'æ˜¯' if force else 'å¦'})")
    
    for pattern in cache_patterns:
        for cache_file in cache_dir.glob(pattern):
            should_clean = force
            
            if not should_clean and pdf_path and pdf_path.exists():
                try:
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    pdf_mtime = pdf_path.stat().st_mtime
                    cache_mtime = cache_file.stat().st_mtime
                    should_clean = cache_mtime < pdf_mtime
                except Exception:
                    should_clean = True  # å‡ºé”™æ—¶æ¸…ç†
            
            if should_clean:
                try:
                    file_size = cache_file.stat().st_size
                    cache_file.unlink()
                    cleaned_count += 1
                    total_size_cleaned += file_size
                    logging.info(f"ğŸ—‘ï¸  åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file.name} ({file_size/1024:.1f}KB)")
                except Exception as e:
                    logging.warning(f"âš ï¸  æ¸…ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
    
    if cleaned_count > 0:
        logging.info(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_size_cleaned/1024:.1f}KB ç©ºé—´")
    else:
        logging.info("ğŸ’¾ æ— éœ€æ¸…ç†ç¼“å­˜æ–‡ä»¶")

# ========= è¯»å–é…ç½® ========= #
def load_config(config_file: str = None) -> Dict:
    """å®‰å…¨åŠ è½½é…ç½®æ–‡ä»¶"""
    ROOT = Path(__file__).resolve().parent
    
    if config_file:
        config_path = Path(config_file)
        if not config_path.is_absolute():
            config_path = ROOT / config_file
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œå°è¯•ä»å‘½ä»¤è¡Œå‚æ•°è·å–
        if len(sys.argv) > 1:
            config_path = Path(sys.argv[1])
            if not config_path.is_absolute():
                config_path = ROOT / sys.argv[1]
        else:
            # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„config.json
            config_path = ROOT / "config.json"
    
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    try:
        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)
        
        # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        required_keys = ["api", "paths"]
        for key in required_keys:
            if key not in config:
                raise KeyError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€é¡¹: {key}")
        
        # éªŒè¯APIé…ç½®
        api_keys = ["API_URL", "API_KEY", "LLM_MODEL"]
        for key in api_keys:
            if key not in config["api"]:
                raise KeyError(f"APIé…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
        
        # éªŒè¯è·¯å¾„é…ç½®
        path_keys = ["pdf", "output_dir", "big_md_name"]
        for key in path_keys:
            if key not in config["paths"]:
                raise KeyError(f"è·¯å¾„é…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
        
        return config
    except json.JSONDecodeError as e:
        raise ValueError(f"é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

ROOT = Path(__file__).resolve().parent

# äº¤äº’å¼è·å–é…ç½®æ–‡ä»¶è·¯å¾„
def get_config_path() -> str:
    """äº¤äº’å¼è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    if len(sys.argv) > 1:
        return sys.argv[1]
    
    print("è¯·é€‰æ‹©é…ç½®æ–‡ä»¶:")
    print("1. ä½¿ç”¨é»˜è®¤é…ç½® (config.json)")
    print("2. è¾“å…¥è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„")
    
    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
        if choice == "1":
            return "config.json"
        elif choice == "2":
            config_path = input("è¯·è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„: ").strip()
            if config_path:
                return config_path
            else:
                print("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")

config_file = get_config_path()
CONFIG: Dict = load_config(config_file)
print(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}")

API_URL      = CONFIG["api"]["API_URL"]
API_KEY      = CONFIG["api"]["API_KEY"]
LLM_MODEL    = CONFIG["api"]["LLM_MODEL"]
TEMPERATURE  = CONFIG["api"].get("temperature", 0.2)

PDF_PATH     = Path(CONFIG["paths"]["pdf"])
OUT_DIR      = Path(CONFIG["paths"]["output_dir"])
BIG_MD_NAME  = CONFIG["paths"]["big_md_name"]

# æ–°å¢é…ç½®é¡¹ï¼šæ¯æ‰¹å¤„ç†çš„é¡µæ•°
PAGES_PER_BATCH = CONFIG.get("pages_per_batch", 10)  # é»˜è®¤æ¯10é¡µç¿»è¯‘ä¸€æ¬¡

# éªŒè¯PDFæ–‡ä»¶å­˜åœ¨
if not PDF_PATH.exists():
    raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {PDF_PATH}")

# ========= ç¼“å­˜ç®¡ç† ========= #

# åˆå§‹åŒ–style_cacheç›¸å…³
STYLE_FILE = OUT_DIR / "style_cache.txt"
style_cache = ""
if STYLE_FILE.exists():
    try:
        style_cache = STYLE_FILE.read_text(encoding="utf-8").strip()
    except Exception as e:
        logging.warning(f"è¯»å–é£æ ¼ç¼“å­˜å¤±è´¥: {e}")
        style_cache = ""

# ========= å¸¸é‡ ========= #
HEAD_SEP  = "\n" + ("â”€"*80) + "\n"
TAG_PAT   = re.compile(r"<c\d+>(.*?)</c\d+>", re.S)
# NEWTERM_PATå·²ç§»é™¤ï¼Œä¸å†å¤„ç†æœ¯è¯­è¡¨

# ========= æ—¥å¿—é…ç½® ========= #
class ColoredFormatter(logging.Formatter):
    """å½©è‰²æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    # é¢œè‰²ä»£ç 
    COLORS = {
        'DEBUG': '\033[36m',    # é’è‰²
        'INFO': '\033[32m',     # ç»¿è‰²
        'WARNING': '\033[33m',  # é»„è‰²
        'ERROR': '\033[31m',    # çº¢è‰²
        'CRITICAL': '\033[35m', # ç´«è‰²
        'RESET': '\033[0m'      # é‡ç½®
    }
    
    def format(self, record):
        # æ·»åŠ é¢œè‰²
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        reset = self.COLORS['RESET']
        
        # æ ¼å¼åŒ–æ—¶é—´
        record.asctime = self.formatTime(record, self.datefmt)
        
        # æ ¹æ®æ—¥å¿—çº§åˆ«ä½¿ç”¨ä¸åŒæ ¼å¼
        if record.levelname == 'INFO':
            if '===' in record.getMessage():
                # æ‰¹æ¬¡å¤„ç†æ ‡é¢˜
                return f"{color}{'='*60}{reset}\n{color}[{record.asctime}] {record.getMessage()}{reset}\n{color}{'='*60}{reset}"
            elif 'è¿›åº¦:' in record.getMessage():
                # è¿›åº¦ä¿¡æ¯
                return f"{color}[{record.asctime}] ğŸ“Š {record.getMessage()}{reset}"
            elif 'ç¼“å­˜' in record.getMessage():
                # ç¼“å­˜ç›¸å…³
                return f"{color}[{record.asctime}] ğŸ’¾ {record.getMessage()}{reset}"
            elif 'å®Œæˆ' in record.getMessage() or 'æˆåŠŸ' in record.getMessage():
                # æˆåŠŸä¿¡æ¯
                return f"{color}[{record.asctime}] âœ… {record.getMessage()}{reset}"
            else:
                return f"{color}[{record.asctime}] â„¹ï¸  {record.getMessage()}{reset}"
        elif record.levelname == 'WARNING':
            return f"{color}[{record.asctime}] âš ï¸  {record.getMessage()}{reset}"
        elif record.levelname == 'ERROR':
            return f"{color}[{record.asctime}] âŒ {record.getMessage()}{reset}"
        else:
            return f"{color}[{record.asctime}] [{record.levelname}] {record.getMessage()}{reset}"

def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = logging.DEBUG if verbose else logging.INFO
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(ColoredFormatter(datefmt="%H:%M:%S"))
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=level,
        handlers=[console_handler],
        force=True
    )

def log_progress(current: int, total: int, prefix: str = "è¿›åº¦", suffix: str = ""):
    """æ˜¾ç¤ºè¿›åº¦æ¡"""
    percent = (current / total) * 100
    bar_length = 30
    filled_length = int(bar_length * current // total)
    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
    
    logging.info(f"{prefix}: [{bar}] {percent:.1f}% ({current}/{total}) {suffix}")

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
setup_logging(verbose=CONFIG.get('verbose_logging', False))

# ========= è¾…åŠ©å‡½æ•° ========= #
# ç§»é™¤äº†detect_titleså‡½æ•°ï¼Œç°åœ¨ç”±LLMè´Ÿè´£è¯†åˆ«æ ‡é¢˜å’Œé¡µçœ‰é¡µç 

def ensure_sentence_completion(text: str, next_batch_text: str = "") -> str:
    """æ™ºèƒ½å¥å­å®Œæ•´æ€§å¤„ç†ï¼šå¦‚æœå½“å‰batchæœ€åä¸€å¥æ²¡æœ‰ç»“æŸï¼Œåªè¡¥å……å®Œæ•´è¿™ä¸ªåŠå¥"""
    if not text.strip():
        return text
    
    # ç§»é™¤æœ«å°¾ç©ºç™½å­—ç¬¦
    text = text.rstrip()
    
    # å¦‚æœæ²¡æœ‰ä¸‹ä¸€æ‰¹æ¬¡å†…å®¹ï¼Œç›´æ¥è¿”å›
    if not next_batch_text.strip():
        return text
    
    # æ£€æŸ¥æœ€åä¸€å¥æ˜¯å¦å®Œæ•´ï¼ˆä»¥å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€å¼•å·ç­‰ç»“æŸï¼‰
    sentence_endings = r'[.!?"\'\'\"\)\]\}]\s*$'
    
    # å¦‚æœæœ€åä¸€å¥å·²ç»å®Œæ•´ï¼Œç›´æ¥è¿”å›
    if re.search(sentence_endings, text):
        return text
    
    # å¦‚æœæœ€åä¸€å¥æ²¡æœ‰å®Œæ•´ï¼Œä»ä¸‹ä¸€æ‰¹æ¬¡ä¸­æ‰¾åˆ°å¥å­ç»“æŸä½ç½®
    next_text = next_batch_text.strip()
    
    # ä¼˜å…ˆæ ¹æ®ç©ºè¡Œï¼ˆæ®µè½è¾¹ç•Œï¼‰æŸ¥æ‰¾å¥å­ç»“æŸä½ç½®
    # é¦–å…ˆæŸ¥æ‰¾ç¬¬ä¸€ä¸ªç©ºè¡Œï¼ˆåŒæ¢è¡Œç¬¦ï¼‰ï¼Œè¿™é€šå¸¸è¡¨ç¤ºæ®µè½ç»“æŸ
    paragraph_end_match = re.search(r'\n\s*\n', next_text)
    
    if paragraph_end_match:
        # æ‰¾åˆ°æ®µè½ç»“æŸä½ç½®ï¼Œè¡¥å……åˆ°æ®µè½ç»“æŸ
        end_pos = paragraph_end_match.start()
        completion = next_text[:end_pos].rstrip()
        
        # è®°å½•è¡¥å……çš„å†…å®¹é•¿åº¦ï¼Œç”¨äºæ—¥å¿—
        logging.info(f"ğŸ“ æ£€æµ‹åˆ°æœªå®Œæ•´å¥å­ï¼Œæ ¹æ®æ®µè½è¾¹ç•Œè¡¥å…… {len(completion)} ä¸ªå­—ç¬¦å®Œæˆå¥å­")
        
        return text + completion
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½è¾¹ç•Œï¼Œå†æŸ¥æ‰¾æ ‡ç‚¹ç¬¦å·ç»“æŸä½ç½®
        sentence_end_match = re.search(r'[.!?"\'\'\"\)\]\}]', next_text)
        
        if sentence_end_match:
            # æ‰¾åˆ°å¥å­ç»“æŸä½ç½®ï¼Œåªè¡¥å……åˆ°å¥å­ç»“æŸ
            end_pos = sentence_end_match.end()
            completion = next_text[:end_pos]
            
            # è®°å½•è¡¥å……çš„å†…å®¹é•¿åº¦ï¼Œç”¨äºæ—¥å¿—
            logging.info(f"ğŸ“ æ£€æµ‹åˆ°æœªå®Œæ•´å¥å­ï¼Œæ ¹æ®æ ‡ç‚¹ç¬¦å·è¡¥å…… {len(completion)} ä¸ªå­—ç¬¦å®Œæˆå¥å­")
            
            return text + completion
        else:
            # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåªè¡¥å……ä¸€å°éƒ¨åˆ†å†…å®¹ï¼ˆæœ€å¤š100å­—ç¬¦ï¼‰
            max_supplement = min(100, len(next_text))
            completion = next_text[:max_supplement]
            
            logging.info(f"ğŸ“ æœªæ‰¾åˆ°æ˜ç¡®å¥å­ç»“æŸï¼Œè¡¥å…… {len(completion)} ä¸ªå­—ç¬¦")
            
            return text + completion

def wrap_batch_with_tags(raw_text: str) -> str:
    """æŠŠæ‰¹æ¬¡åŸæ–‡æŒ‰ç©ºè¡Œåˆ†æ®µï¼ŒåŠ  <c1>â€¦</c1> æ ‡ç­¾ï¼Œè®©LLMè‡ªè¡Œè¯†åˆ«æ ‡é¢˜å’Œé¡µçœ‰é¡µç """
    segments = [seg.strip() for seg in re.split(r"\n\s*\n", raw_text) if seg.strip()]
    tagged = []
    
    for idx, seg in enumerate(segments, start=1):
        # ä¸å†é¢„å…ˆæ ‡è®°æ ‡é¢˜ï¼Œè®©LLMè‡ªè¡Œè¯†åˆ«å’Œå¤„ç†
        tagged.append(f"<c{idx}>{seg}</c{idx}>")
    
    return "\n\n".join(tagged)

def strip_tags(llm_output: str, keep_missing: bool = True):
    """æ¸…æ´— LLM è¾“å‡º & æ”¶é›†ç¼ºå¤±æ®µï¼Œä¼˜åŒ–é¡µçœ‰é¡µè„šè¯†åˆ«é€»è¾‘"""
    paragraphs = TAG_PAT.findall(llm_output)

    miss_list, clean_paras = [], []
    for idx, p in enumerate(paragraphs, start=1):
        content = p.strip()
        
        if content == "{{MISSING}}":
            miss_list.append(f"c{idx:03d}")
            if keep_missing:
                clean_paras.append("{{MISSING}}")
        elif content == "":
            # è·³è¿‡å®Œå…¨ç©ºçš„å†…å®¹
            pass
        elif _is_header_footer_content(content):
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„é¡µçœ‰é¡µè„šè¯†åˆ«é€»è¾‘
            pass
        else:
            clean_paras.append(content)

    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…å¤šä½™çš„ç©ºè¡Œ
    clean_paras = [para for para in clean_paras if para.strip()]
    pure_text = "\n\n".join(clean_paras)
    # æœ¯è¯­è¡¨åŠŸèƒ½å·²ç§»é™¤ï¼Œä¸å†å¤„ç†æœ¯è¯­è¡¨å†…å®¹
    return pure_text, "", miss_list

def _is_header_footer_content(content: str) -> bool:
    """æ™ºèƒ½è¯†åˆ«é¡µçœ‰é¡µè„šå†…å®¹ï¼Œé¿å…è¯¯åˆ¤æ­£å¸¸æ–‡æœ¬"""
    # æ˜ç¡®çš„é¡µçœ‰é¡µè„šæ ‡è®°
    if content.startswith("[é¡µçœ‰é¡µè„š]") or content.startswith("[ç›®å½•]"):
        return True
    
    # é¡µç æ¨¡å¼ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
    page_patterns = [
        r'^Page\s+\d+\s+of\s+\d+$',  # "Page 1 of 506"
        r'^ç¬¬\d+é¡µ/å…±\d+é¡µ$',        # "ç¬¬1é¡µ/å…±506é¡µ"
        r'^\d+\s*/\s*\d+$',         # "1/506"
        r'^\d+$'                    # å•ç‹¬çš„æ•°å­—ï¼ˆä½†è¦å°å¿ƒï¼Œå¯èƒ½æ˜¯æ­£æ–‡ï¼‰
    ]
    
    for pattern in page_patterns:
        if re.match(pattern, content.strip(), re.IGNORECASE):
            return True
    
    # ç½‘å€å’Œé‚®ç®±æ¨¡å¼
    if re.search(r'https?://|www\.|@.*\.(com|org|net)', content, re.IGNORECASE):
        return True
    
    # ç‰ˆæƒä¿¡æ¯
    if re.search(r'copyright|Â©|ç‰ˆæƒæ‰€æœ‰|all rights reserved', content, re.IGNORECASE):
        return True
    
    # ä½œè€…ä¿¡æ¯é‡å¤ï¼ˆä½†è¦è°¨æ…ï¼Œé¿å…è¯¯åˆ¤æ­£æ–‡ä¸­çš„ä½œè€…åï¼‰
    # åªæœ‰å½“å†…å®¹å¾ˆçŸ­ä¸”çœ‹èµ·æ¥åƒé‡å¤çš„ä½œè€…ä¿¡æ¯æ—¶æ‰åˆ¤æ–­ä¸ºé¡µçœ‰é¡µè„š
    if len(content) < 50 and re.search(r'^(author|ä½œè€…)[:ï¼š]?\s*[A-Za-z\s]+$', content, re.IGNORECASE):
        return True
    
    return False

def refresh_style(sample_text: str):
    """è‹¥ style_cache ä¸ºç©ºï¼Œåˆ™ç”¨åŸæ–‡æ ·æœ¬è®© LLM å½’çº³é£æ ¼ï¼›å¦åˆ™è·³è¿‡"""
    global style_cache
    if style_cache:
        return
    
    try:
        prompt_sys = "You are a literary critic. Analyze the writing style concisely."
        # ç¡®ä¿æ ·æœ¬æ–‡æœ¬ä¸è¶…è¿‡2000å­—ç¬¦ï¼Œé¿å…APIé™åˆ¶
        sample_text = sample_text[:2000] if len(sample_text) > 2000 else sample_text
        prompt_user = f"""Summarize the narrative voice, tone, humor level and sentence rhythm of the following English text in 80 words:

{sample_text}"""
        
        style_cache = call_llm(prompt_sys, prompt_user)
        if style_cache:
            STYLE_FILE.write_text(style_cache, encoding="utf-8")
            logging.info("é£æ ¼åˆ†æå®Œæˆå¹¶å·²ç¼“å­˜")
        else:
            raise ValueError("é£æ ¼åˆ†æè¿”å›ç©ºç»“æœ")
    except Exception as e:
        logging.error(f"é£æ ¼åˆ†æå¤±è´¥: {e}")
        style_cache = "é»˜è®¤é£æ ¼ï¼šä¿æŒåŸæ–‡çš„å™äº‹èŠ‚å¥å’Œè¯­è°ƒ"

# æœ¯è¯­è¡¨ç›¸å…³å‡½æ•°å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨æœ¯è¯­è¡¨åŠŸèƒ½

def call_llm(prompt_sys: str, prompt_user: str, max_retries: int = 3, timeout: int = 120) -> str:
    """è°ƒç”¨LLM APIï¼Œå¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†"""
    global total_cost, total_input_tokens, total_output_tokens
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": LLM_MODEL,
        "temperature": TEMPERATURE,
        "messages": [
            {"role": "system", "content": prompt_sys},
            {"role": "user", "content": prompt_user}
        ]
    }
    
    last_error = None
    for attempt in range(max_retries):
        try:
            logging.info(f"ğŸ¤– è°ƒç”¨LLM API - æ¨¡å‹: {LLM_MODEL} (å°è¯• {attempt+1}/{max_retries})")
            logging.debug(f"ç³»ç»Ÿæç¤ºé•¿åº¦: {len(prompt_sys)} å­—ç¬¦")
            logging.debug(f"ç”¨æˆ·è¾“å…¥é•¿åº¦: {len(prompt_user)} å­—ç¬¦")
            
            resp = requests.post(API_URL, headers=headers, json=payload, timeout=timeout)
            resp.raise_for_status()
            
            result = resp.json()
            if "choices" not in result or not result["choices"]:
                raise ValueError("APIè¿”å›æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ")
            
            content = result["choices"][0]["message"]["content"]
            if not content or not content.strip():
                raise ValueError("APIè¿”å›å†…å®¹ä¸ºç©º")
            
            logging.info(f"âœ… LLMå“åº”æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è®°å½•tokenä½¿ç”¨æƒ…å†µå’Œè®¡ç®—æˆæœ¬
            if "usage" in result and result["usage"]:
                usage = result["usage"]
                input_tokens = usage.get('prompt_tokens', 0)
                output_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', 0)
                
                # ç´¯è®¡tokenç»Ÿè®¡
                total_input_tokens += input_tokens
                total_output_tokens += output_tokens
                
                # è®¡ç®—æˆæœ¬ï¼ˆå¦‚æœå¯ç”¨äº†æˆæœ¬è·Ÿè¸ªï¼‰
                if CONFIG.get('pricing', {}).get('enable_cost_tracking', False):
                    pricing = CONFIG.get('pricing', {})
                    input_price_per_1k = pricing.get('input_price_per_1k_tokens', 0)
                    output_price_per_1k = pricing.get('output_price_per_1k_tokens', 0)
                    currency = pricing.get('currency', 'USD')
                    
                    batch_input_cost = (input_tokens / 1000) * input_price_per_1k
                    batch_output_cost = (output_tokens / 1000) * output_price_per_1k
                    batch_total_cost = batch_input_cost + batch_output_cost
                    
                    total_cost += batch_total_cost
                    
                    logging.info(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥{input_tokens} + è¾“å‡º{output_tokens} = æ€»è®¡{total_tokens}")
                    logging.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: {batch_total_cost:.4f} {currency} (è¾“å…¥: {batch_input_cost:.4f} + è¾“å‡º: {batch_output_cost:.4f})")
                    logging.info(f"ğŸ’³ ç´¯è®¡æˆæœ¬: {total_cost:.4f} {currency}")
                else:
                    logging.info(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥{input_tokens} + è¾“å‡º{output_tokens} = æ€»è®¡{total_tokens}")
            
            return content.strip()
            
        except requests.exceptions.Timeout:
            last_error = f"è¯·æ±‚è¶…æ—¶({timeout}ç§’)"
            logging.warning(f"LLMè°ƒç”¨è¶…æ—¶({attempt+1}/{max_retries})")
        except requests.exceptions.ConnectionError:
            last_error = "ç½‘ç»œè¿æ¥é”™è¯¯"
            logging.warning(f"LLMè°ƒç”¨ç½‘ç»œé”™è¯¯({attempt+1}/{max_retries})")
        except requests.exceptions.HTTPError as e:
            last_error = f"HTTPé”™è¯¯: {e.response.status_code}"
            logging.warning(f"LLMè°ƒç”¨HTTPé”™è¯¯({attempt+1}/{max_retries}): {e}")
        except (KeyError, ValueError, json.JSONDecodeError) as e:
            last_error = f"å“åº”è§£æé”™è¯¯: {e}"
            logging.warning(f"LLMè°ƒç”¨è§£æé”™è¯¯({attempt+1}/{max_retries}): {e}")
        except Exception as e:
            last_error = f"æœªçŸ¥é”™è¯¯: {e}"
            logging.warning(f"LLMè°ƒç”¨æœªçŸ¥é”™è¯¯({attempt+1}/{max_retries}): {e}")
        
        if attempt < max_retries - 1:
            wait_time = min(5 * (2 ** attempt), 30)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§30ç§’
            logging.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    raise RuntimeError(f"LLMè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}")

# ========= å‡†å¤‡è¾“å‡ºç›®å½• ========= #
try:
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    CHAP_DIR = OUT_DIR / "chap_md"
    CHAP_DIR.mkdir(exist_ok=True)
    RAW_CONTENT_DIR = OUT_DIR / "raw_content"
    RAW_CONTENT_DIR.mkdir(exist_ok=True)
    logging.info(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {OUT_DIR}")
    logging.info(f"åŸå§‹å†…å®¹ç›®å½•å·²å‡†å¤‡: {RAW_CONTENT_DIR}")
except Exception as e:
    raise RuntimeError(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")

# æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†è¿‡æœŸç¼“å­˜
if CONFIG.get("clean_cache_on_start", True):
    logging.info("=== æ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶ ===")
    clean_cache_files(RAW_CONTENT_DIR, PDF_PATH, force=False)

# æœ¯è¯­è¡¨åŠŸèƒ½å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨æœ¯è¯­è¡¨
logging.info("æœ¯è¯­è¡¨åŠŸèƒ½å·²ç¦ç”¨ï¼Œä½¿ç”¨promptçº¦æŸä¸“æœ‰åè¯å¤„ç†")

# ========= è§£ææ•´æœ¬ PDFï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰========= #
def get_pdf_text_with_cache(pdf_path: Path, cache_dir: Path) -> str:
    """è·å–PDFæ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    # ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
    pdf_name = pdf_path.stem
    cache_file = cache_dir / f"{pdf_name}_text_cache.txt"
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    if cache_file.exists():
        try:
            pdf_mtime = pdf_path.stat().st_mtime
            cache_mtime = cache_file.stat().st_mtime
            
            # å¦‚æœç¼“å­˜æ–‡ä»¶æ¯”PDFæ–‡ä»¶æ–°ï¼Œä½¿ç”¨ç¼“å­˜
            if cache_mtime >= pdf_mtime:
                cache_size = cache_file.stat().st_size
                logging.info(f"ğŸ’¾ ä½¿ç”¨PDFæ–‡æœ¬ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB)")
                cached_text = cache_file.read_text(encoding="utf-8")
                logging.info(f"ğŸ“„ ç¼“å­˜æ–‡æœ¬é•¿åº¦: {len(cached_text)} å­—ç¬¦")
                return cached_text
            else:
                logging.info("ğŸ“ PDFæ–‡ä»¶å·²æ›´æ–°ï¼Œé‡æ–°æå–æ–‡æœ¬")
        except Exception as e:
            logging.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}ï¼Œå°†é‡æ–°æå–PDFæ–‡æœ¬")
    
    # æå–PDFæ–‡æœ¬
    logging.info(f"ğŸ” å¼€å§‹æå–PDFæ–‡æœ¬: {pdf_path.name}")
    import time
    start_time = time.time()
    
    # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬ï¼Œæ›´å¥½åœ°å¤„ç†æ®µè½ç»“æ„
    doc = fitz.open(str(pdf_path))
    pages = []
    
    for page_num in range(doc.page_count):
        page = doc[page_num]
        # ä½¿ç”¨sort=Trueè·å¾—æ›´å¥½çš„é˜…è¯»é¡ºåº
        # ä½¿ç”¨blocksæ¨¡å¼è·å¾—æ›´å¥½çš„æ®µè½ç»“æ„
        blocks = page.get_text("blocks", sort=True)
        page_text = ""
        
        for block in blocks:
            if len(block) >= 5 and block[4]:  # æ–‡æœ¬å—
                block_text = block[4].strip()
                if block_text:
                    page_text += block_text + "\n\n"
        
        pages.append(page_text.rstrip())
    
    doc.close()
    full_text = "\f".join(pages)  # ä¿æŒä¸åŸæ¥çš„åˆ†é¡µç¬¦ä¸€è‡´
    
    extract_time = time.time() - start_time
    
    # ä¿å­˜åˆ°ç¼“å­˜
    try:
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file.write_text(full_text, encoding="utf-8")
        cache_size = cache_file.stat().st_size
        logging.info(f"ğŸ’¾ PDFæ–‡æœ¬å·²ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB, è€—æ—¶{extract_time:.1f}ç§’)")
        logging.info(f"ğŸ“„ æå–æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    except Exception as e:
        logging.warning(f"ä¿å­˜æ–‡æœ¬ç¼“å­˜å¤±è´¥: {e}")
    
    return full_text

logging.info(f"å¼€å§‹åŠ è½½PDFæ–‡ä»¶: {PDF_PATH}")
try:
    # è·å–PDFæ–‡æœ¬ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    full_text = get_pdf_text_with_cache(PDF_PATH, RAW_CONTENT_DIR)
    if not full_text or not full_text.strip():
        raise ValueError("PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
    
    pages = re.split(r"\f", full_text)                           # PyMuPDF æŒ‰ formfeed åˆ†é¡µ
    if pages and pages[-1] == "":
        pages.pop()
    
    logging.info(f"PDFåŠ è½½å®Œæˆï¼Œå…±{len(pages)}é¡µ")
    
    # éªŒè¯é¡µæ•°é…ç½®
    max_page = len(pages)
    if PAGES_PER_BATCH < 1:
        raise ValueError(f"æ¯æ‰¹é¡µæ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰è®¾ç½®: {PAGES_PER_BATCH}")
    
    logging.info(f"å°†æŒ‰æ¯{PAGES_PER_BATCH}é¡µè¿›è¡Œåˆ†æ‰¹ç¿»è¯‘ï¼ŒPDFå…±{max_page}é¡µ")
    
except Exception as e:
    raise RuntimeError(f"PDFå¤„ç†å¤±è´¥: {e}")

# ========= æ±‡æ€»è¾“å‡º ========= #
MISSING_DICT = {}
WARNING_DICT = {}  # æ”¶é›†æ®µè½æ•°é‡å·®å¼‚ç­‰è­¦å‘Šä¿¡æ¯
big_md_parts = []
total_pages = len(pages)
total_batches = (total_pages + PAGES_PER_BATCH - 1) // PAGES_PER_BATCH  # å‘ä¸Šå–æ•´
processed_batches = 0

logging.info(f"=== å¼€å§‹åˆ†æ‰¹ç¿»è¯‘å¤„ç† ===")
logging.info(f"æ€»æ‰¹æ¬¡: {total_batches} | æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH} | æ€»é¡µæ•°: {total_pages}")

def get_batch_text_with_cache(pages: List[str], batch_num: int, p_start: int, p_end: int, cache_dir: Path) -> str:
    """è·å–æ‰¹æ¬¡æ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    batch_id = f"batch_{batch_num:03d}"
    cache_file = cache_dir / f"{batch_id}_raw_text.txt"
    
    # æ£€æŸ¥æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    if cache_file.exists():
        try:
            cached_text = cache_file.read_text(encoding="utf-8")
            if cached_text.strip():
                cache_size = cache_file.stat().st_size
                logging.debug(f"ğŸ’¾ ä½¿ç”¨æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB)")
                logging.debug(f"ğŸ“„ æ‰¹æ¬¡æ–‡æœ¬é•¿åº¦: {len(cached_text)} å­—ç¬¦")
                return cached_text
        except Exception as e:
            logging.warning(f"è¯»å–æ‰¹æ¬¡ç¼“å­˜å¤±è´¥: {e}")
    
    # ä»pagesæ•°ç»„ä¸­æå–æ–‡æœ¬
    raw_eng = "\n".join(pages[p_start-1:p_end])  # é¡µç ä» 1 å¼€å§‹
    
    # ä¿å­˜æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    try:
        cache_file.write_text(raw_eng, encoding="utf-8")
        cache_size = cache_file.stat().st_size
        logging.debug(f"ğŸ’¾ æ‰¹æ¬¡æ–‡æœ¬å·²ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB)")
        logging.debug(f"ğŸ“„ æ‰¹æ¬¡æ–‡æœ¬é•¿åº¦: {len(raw_eng)} å­—ç¬¦")
    except Exception as e:
        logging.warning(f"ä¿å­˜æ‰¹æ¬¡ç¼“å­˜å¤±è´¥: {e}")
    
    return raw_eng

# æŒ‰é¡µæ•°åˆ†æ‰¹å¤„ç†
for batch_num in range(1, total_batches + 1):
    processed_batches += 1
    
    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„é¡µç èŒƒå›´
    p_start = (batch_num - 1) * PAGES_PER_BATCH + 1
    p_end = min(batch_num * PAGES_PER_BATCH, total_pages)
    batch_id = f"batch_{batch_num:03d}"
    
    logging.info(f"=== å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (é¡µ {p_start}-{p_end}) ===")
    log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", f"å½“å‰: æ‰¹æ¬¡{batch_num}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¿»è¯‘ç»“æœç¼“å­˜
    batch_md_file = CHAP_DIR / f"{batch_id}.md"
    if batch_md_file.exists():
        try:
            cached_content = batch_md_file.read_text(encoding="utf-8")
            if cached_content.strip():
                logging.info(f"ğŸ’¾ æ‰¹æ¬¡ {batch_num} å·²å­˜åœ¨ç¿»è¯‘ç»“æœï¼Œè·³è¿‡å¤„ç†")
                big_md_parts.append(cached_content)
                log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "ä½¿ç”¨ç¼“å­˜")
                continue
        except Exception as e:
            logging.warning(f"è¯»å–æ‰¹æ¬¡ç¿»è¯‘ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°å¤„ç†")
    
    try:
        # è·å–æ‰¹æ¬¡æ–‡æœ¬ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        raw_eng = get_batch_text_with_cache(pages, batch_num, p_start, p_end, RAW_CONTENT_DIR)
        
        if not raw_eng.strip():
            logging.warning(f"ğŸ“„ æ‰¹æ¬¡{batch_num}å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
            MISSING_DICT[batch_id] = ["æ•´æ‰¹ç¼ºå¤±"]
            log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "å†…å®¹ä¸ºç©º")
            continue
        
        # æ™ºèƒ½å¥å­å®Œæ•´æ€§å¤„ç†ï¼šå¦‚æœå½“å‰æ‰¹æ¬¡æœ€åä¸€å¥æ²¡æœ‰ç»“æŸï¼Œä»ä¸‹ä¸€æ‰¹æ¬¡è¡¥å……å®Œæ•´
        if batch_num < total_batches:  # ä¸æ˜¯æœ€åä¸€ä¸ªæ‰¹æ¬¡
            # è·å–ä¸‹ä¸€æ‰¹æ¬¡çš„æ–‡æœ¬ç”¨äºå¥å­å®Œæ•´æ€§æ£€æŸ¥
            next_p_start = batch_num * PAGES_PER_BATCH + 1
            next_p_end = min((batch_num + 1) * PAGES_PER_BATCH, total_pages)
            try:
                next_batch_text = get_batch_text_with_cache(pages, batch_num + 1, next_p_start, next_p_end, RAW_CONTENT_DIR)
                # åº”ç”¨æ™ºèƒ½å¥å­å®Œæ•´æ€§å¤„ç†
                raw_eng = ensure_sentence_completion(raw_eng, next_batch_text)
            except Exception as e:
                logging.warning(f"è·å–ä¸‹ä¸€æ‰¹æ¬¡æ–‡æœ¬å¤±è´¥ï¼Œè·³è¿‡å¥å­å®Œæ•´æ€§å¤„ç†: {e}")
                # å¦‚æœè·å–ä¸‹ä¸€æ‰¹æ¬¡å¤±è´¥ï¼Œä»ç„¶ä½¿ç”¨åŸå§‹æ–‡æœ¬
                raw_eng = ensure_sentence_completion(raw_eng)
        
        tagged_eng = wrap_batch_with_tags(raw_eng)
        
        # æ£€æŸ¥æ ‡ç­¾æ•°é‡æ˜¯å¦åˆç†
        tag_count = len(re.findall(r'<c\d+>', tagged_eng))
        if tag_count == 0:
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æœªèƒ½æ­£ç¡®åˆ†æ®µ")
        else:
            logging.info(f"ğŸ“ æ‰¹æ¬¡{batch_num}åˆ†ä¸º{tag_count}ä¸ªæ®µè½ï¼Œæ–‡æœ¬é•¿åº¦: {len(raw_eng)} å­—ç¬¦")

        # --- è·å–é£æ ¼ä¿¡æ¯ ---
        if not style_cache:
            # ä»ä¸­é—´éšæœºå–æ ·ä½œä¸ºé£æ ¼åˆ†ææ ·æœ¬ï¼Œé¿å…å–åˆ°å‰è¨€ã€ä½œè€…çš„è¯ç­‰å†…å®¹
            text_length = len(raw_eng)
            sample_length = min(5000, text_length)  # æ ·æœ¬é•¿åº¦ä¸è¶…è¿‡æ–‡æœ¬æ€»é•¿åº¦
            
            if text_length > sample_length:
                # ä»æ–‡æœ¬çš„ä¸­é—´éƒ¨åˆ†éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
                # é¿å…å‰20%å’Œå20%çš„å†…å®¹ï¼Œä¸»è¦ä»ä¸­é—´60%çš„éƒ¨åˆ†å–æ ·
                start_range_begin = int(text_length * 0.2)
                start_range_end = int(text_length * 0.8) - sample_length
                
                if start_range_end > start_range_begin:
                    start_pos = random.randint(start_range_begin, start_range_end)
                    sample_text = raw_eng[start_pos:start_pos + sample_length]
                    logging.info(f"ğŸ“ ä»æ–‡æœ¬ä¸­é—´éšæœºå–æ ·è¿›è¡Œé£æ ¼åˆ†æ (ä½ç½®: {start_pos}-{start_pos + sample_length})")
                else:
                    # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œå°±å–ä¸­é—´éƒ¨åˆ†
                    start_pos = max(0, (text_length - sample_length) // 2)
                    sample_text = raw_eng[start_pos:start_pos + sample_length]
                    logging.info(f"ğŸ“ ä»æ–‡æœ¬ä¸­é—´å–æ ·è¿›è¡Œé£æ ¼åˆ†æ (ä½ç½®: {start_pos}-{start_pos + sample_length})")
            else:
                # æ–‡æœ¬é•¿åº¦ä¸è¶³5000å­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨æ–‡æœ¬
                sample_text = raw_eng
                logging.info("ğŸ“ æ–‡æœ¬è¾ƒçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹è¿›è¡Œé£æ ¼åˆ†æ")
            
            refresh_style(sample_text)
        
        # --- æ„é€ ç³»ç»Ÿæç¤º ---
        system_prompt = textwrap.dedent( f"""
            ä½ æ˜¯ä¸€åèµ„æ·±æ–‡å­¦è¯‘è€…ï¼Œç²¾é€šä¸­è‹±æ–‡å­¦åˆ›ä½œï¼Œéœ€æŠŠä¸‹æ–¹å°è¯´ç²¾å‡†ã€ä¼˜é›…åœ°è¯‘æˆç°ä»£ä¸­æ–‡ï¼Œè¿½æ±‚æ–‡å­¦æ€§ä¸å¯è¯»æ€§çš„å®Œç¾å¹³è¡¡ã€‚

            ================ ä»»åŠ¡è¦æ±‚ ================
            1. **é€æ®µè½å¯¹é½**  
            â€¢ æºè‹±æ–‡ç”¨ <cN>â€¦</cN> æ ‡ç­¾åŒ…è£¹ï¼ˆè„šæœ¬å·²è‡ªåŠ¨åŠ ï¼‰ã€‚  
            â€¢ ä½ å¿…é¡»ä¸º *æ¯ä¸€ä¸ª* <cN> æ®µè½è¾“å‡ºå¯¹åº” <cN> æ®µè½ï¼Œä¿æŒé¡ºåºä¸€è‡´ã€‚  
            â€¢ ç»ä¸å¯åˆå¹¶ã€å¢åˆ æˆ–è·³è¿‡æ®µè½ã€‚è‹¥ç¡®å®æ— æ³•ç¿»è¯‘ï¼ŒåŸæ–‡ç”¨ <cN>{{{{MISSING}}}}</cN> åŸæ ·æŠ„å†™ã€‚

            2. **ä¸çœç•¥**ï¼ˆæå…¶é‡è¦ï¼ï¼‰  
            â€¢ è¯‘æ–‡è¡Œæ•° â‰ˆ æºè¡Œæ•°ã€‚  
            â€¢ **å¿…é¡»ç¿»è¯‘æ¯ä¸€ä¸ªæ®µè½**ï¼šç‰¹åˆ«æ³¨æ„æœ€åä¸€ä¸ªæ®µè½ï¼Œç»ä¸èƒ½é—æ¼ï¼ 
            â€¢ **å¼ºåˆ¶å®Œæ•´æ€§æ£€æŸ¥**ï¼šç¿»è¯‘å®Œæˆåï¼Œå¿…é¡»æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ <cN> æ®µè½éƒ½æœ‰å¯¹åº”è¾“å‡ºã€‚ 
            â€¢ ç»“å°¾è‡ªè¡Œæ‰§è¡Œæ£€æŸ¥ï¼šè‹¥å‘ç°æœ‰æœªè¾“å‡ºçš„ <cX> æ®µï¼Œå¿…é¡»è¡¥ä¸Š <cX>{{{{MISSING}}}}</cX>ã€‚ 
            â€¢ **æœ€åä¸€æ®µç‰¹åˆ«æé†’**ï¼šæœ€åä¸€ä¸ªæ®µè½å¾€å¾€å®¹æ˜“è¢«é—æ¼ï¼Œè¯·åŠ¡å¿…ç¡®ä¿ç¿»è¯‘å®Œæ•´ï¼

            3. **æ™ºèƒ½è¯†åˆ«ä¸å¤„ç†**ï¼ˆé‡è¦ï¼ï¼‰ 
            â€¢ **é¡µçœ‰é¡µè„šæ ‡è®°**ï¼šé‡åˆ°ä»¥ä¸‹å†…å®¹è¾“å‡ºç‰¹æ®Šæ ‡è®° <cN>[é¡µçœ‰é¡µè„š]</cN>ï¼š 
              - é¡µç ä¿¡æ¯ï¼ˆå¦‚"Page 1 of 506"ã€"ç¬¬1é¡µ/å…±506é¡µ"ç­‰ï¼‰ 
              - ä½œè€…ä¿¡æ¯é‡å¤ï¼ˆå¦‚é‚®ç®±åœ°å€ã€ä½œè€…åé‡å¤å‡ºç°ï¼‰ 
              - ç½‘ç«™é“¾æ¥ã€ç‰ˆæƒä¿¡æ¯ 
              - æ˜æ˜¾çš„é¡µçœ‰é¡µè„šé‡å¤å†…å®¹ 
            â€¢ **ç« èŠ‚æ ‡é¢˜è¯†åˆ«ä¸æ ¼å¼ç»Ÿä¸€**ï¼šè¯†åˆ«ä»¥ä¸‹å†…å®¹å¹¶è½¬æ¢ä¸ºç»Ÿä¸€çš„Markdownæ ¼å¼ï¼ˆäºŒçº§æ ‡é¢˜##ï¼‰ï¼š 
               - **æ­£æ–‡ç« èŠ‚**ï¼š"Chapter X"ã€"ç¬¬Xç« "ã€"ç¬¬01ç« "ã€"ç¬¬ä¸€ç« " â†’ ## ç¬¬Xç«  
               - **ç¼–å·ç« èŠ‚**ï¼š"01."ã€"1."ã€"ä¸€ã€"ã€"Chapter 1:"ç­‰ç¼–å·å¼€å¤´çš„æ ‡é¢˜ â†’ ## ç¬¬01ç«  æ ‡é¢˜å†…å®¹ 
               - **ç‰¹æ®Šç« èŠ‚**ï¼š"ç•ªå¤–01."ã€"ç‰¹åˆ«ç¯‡01."ã€"å¤–ä¼ 01." â†’ ## ç•ªå¤–01 æ ‡é¢˜å†…å®¹ã€## ç‰¹åˆ«ç¯‡01 æ ‡é¢˜å†…å®¹ 
               - **åºç« ç»“å°¾**ï¼š"PROLOGUE"ã€"åºç« "ã€"æ¥”å­" â†’ ## åºç« ã€"EPILOGUE"ã€"å°¾å£°"ã€"åè®°" â†’ ## å°¾å£° 
               - **è£…é¥°æ ‡é¢˜**ï¼šå¸¦æœ‰è£…é¥°æ€§ç¬¦å·çš„æ ‡é¢˜ï¼ˆå¦‚"â˜˜ 01.æˆ‘å¿ä¸äº†äº† â˜˜"ã€"â˜…ç¬¬ä¸€ç« â˜…"ï¼‰â†’ å»æ‰è£…é¥°ç¬¦å·åè½¬æ¢ä¸º ## ç¬¬01ç«  æˆ‘å¿ä¸äº†äº† 
               - **å±…ä¸­çŸ­æ ‡é¢˜**ï¼šæ˜æ˜¾çš„ç« èŠ‚æ ‡é¢˜ï¼ˆé€šå¸¸ç‹¬ç«‹æˆè¡Œã€å­—æ•°è¾ƒå°‘ï¼‰â†’ ## æ ‡é¢˜å†…å®¹ 
               - **æ ¼å¼è¦æ±‚**ï¼šç« èŠ‚ç¼–å·ç»Ÿä¸€ä¸ºä¸¤ä½æ•°å­—ï¼ˆå¦‚ç¬¬01ç« ã€ç¬¬02ç« ï¼‰ï¼Œæ ‡é¢˜ä¸ç¼–å·ä¹‹é—´ç”¨ç©ºæ ¼åˆ†éš” 
            â€¢ **ç‰¹æ®Šå†…å®¹å¤„ç†**ï¼š 
              - ä½œè€…çš„è¯ã€å‰è¨€ã€åè®°ç­‰ â†’ ## ä½œè€…çš„è¯
              - ç›®å½•ã€ç´¢å¼•ç­‰ â†’ [ç›®å½•]

            4. **ä¸“æœ‰åè¯å’Œç§°å‘¼å¤„ç†**ï¼ˆé‡è¦ï¼ï¼‰  
            â€¢ **äººåä¿æŒåŸæ–‡**ï¼šæ‰€æœ‰äººåï¼ˆå¦‚ Johnã€Maryã€Somchai ç­‰ï¼‰ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘æˆä¸­æ–‡ã€‚ 
            â€¢ **æ³°è¯­ç§°å‘¼ä¿æŒåŸæ–‡**ï¼šä»¥ä¸‹æ³°è¯­ç§°å‘¼è¯æ±‡å¿…é¡»ä¿æŒåŸæ–‡ï¼Œä¸è¦ç¿»è¯‘ï¼š 
              - Khunï¼ˆà¸„à¸¸à¸“ï¼‰- æ•¬è¯­ç§°å‘¼ï¼Œç›¸å½“äºå…ˆç”Ÿ/å¥³å£« 
              - P'ï¼ˆà¸à¸µà¹ˆï¼‰- å¯¹å¹´é•¿è€…çš„ç§°å‘¼ï¼Œå“¥å“¥/å§å§ 
              - N'ï¼ˆà¸™à¹‰à¸­à¸‡ï¼‰- å¯¹å¹´å¹¼è€…çš„ç§°å‘¼ï¼Œå¼Ÿå¼Ÿ/å¦¹å¦¹ 
              - Phiï¼ˆà¸à¸µà¹ˆï¼‰- P'çš„å®Œæ•´å½¢å¼ 
              - Nongï¼ˆà¸™à¹‰à¸­à¸‡ï¼‰- N'çš„å®Œæ•´å½¢å¼ 
              - Ajarnï¼ˆà¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œï¼‰- è€å¸ˆã€æ•™æˆ 
              - Krub/Krabï¼ˆà¸„à¸£à¸±à¸šï¼‰- ç”·æ€§æ•¬è¯­è¯­å°¾è¯ 
              - Ka/Khaï¼ˆà¸„à¹ˆà¸°/à¸„à¸°ï¼‰- å¥³æ€§æ•¬è¯­è¯­å°¾è¯ 
            â€¢ **å“ç‰Œåä¿æŒåŸæ–‡**ï¼šå“ç‰Œåç§°ä¿æŒåŸæ–‡ä¸ç¿»è¯‘ã€‚

            5. **æ–‡å­¦æ€§ç¿»è¯‘è¦æ±‚**ï¼ˆæ ¸å¿ƒï¼ï¼‰
            â€¢ **è¯­è¨€ç¾æ„Ÿ**ï¼šè¿½æ±‚è¯‘æ–‡çš„éŸµå¾‹æ„Ÿå’ŒèŠ‚å¥æ„Ÿï¼Œé¿å…ç”Ÿç¡¬ç›´è¯‘ï¼Œæ³¨é‡è¯­è¨€çš„æµç•…æ€§å’Œä¼˜ç¾æ€§ã€‚
            â€¢ **æƒ…æ„Ÿä¼ è¾¾**ï¼šæ·±åº¦ç†è§£åŸæ–‡çš„æƒ…æ„Ÿè‰²å½©ï¼Œå‡†ç¡®ä¼ è¾¾äººç‰©çš„å†…å¿ƒä¸–ç•Œã€æƒ…ç»ªå˜åŒ–å’Œå¿ƒç†çŠ¶æ€ã€‚
            â€¢ **ä¿®è¾ä¿ç•™**ï¼šä¿ç•™å¹¶è½¬åŒ–åŸæ–‡çš„ä¿®è¾æ‰‹æ³•ï¼Œå¦‚æ¯”å–»ã€æ‹Ÿäººã€æ’æ¯”ç­‰ï¼Œä½¿å…¶ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ã€‚
            â€¢ **æ„å¢ƒè¥é€ **ï¼šæ³¨é‡è¥é€ æ–‡å­¦æ„å¢ƒï¼Œé€šè¿‡è¯æ±‡é€‰æ‹©å’Œå¥å¼å®‰æ’ï¼Œåˆ›é€ å¯Œæœ‰è¯—æ„çš„é˜…è¯»ä½“éªŒã€‚
            â€¢ **æ–‡åŒ–è½¬æ¢**ï¼šå°†è‹±æ–‡çš„æ–‡åŒ–èƒŒæ™¯å’Œè¡¨è¾¾æ–¹å¼å·§å¦™è½¬æ¢ä¸ºä¸­æ–‡è¯»è€…èƒ½å¤Ÿç†è§£å’Œå…±é¸£çš„å½¢å¼ã€‚
            â€¢ **å±‚æ¬¡ä¸°å¯Œ**ï¼šæ ¹æ®ä¸åŒåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­è¨€é£æ ¼ï¼Œå¦‚æŠ’æƒ…æ®µè½ç”¨è¯ä¼˜ç¾ï¼Œå¯¹è¯éƒ¨åˆ†ç”ŸåŠ¨è‡ªç„¶ï¼Œæå†™æ®µè½ç»†è…»å…¥å¾®ã€‚

            6. **é£æ ¼å®ˆåˆ™**  
            â€¢ ä¿æŒåŸæ–‡é£æ ¼ç‰¹å¾ï¼š{style_cache} 
            â€¢ **ç¬¬ä¸‰äººç§°æ”¹ç¬¬ä¸€äººç§°**ï¼šæ³°è¯­è½¬è¯‘ä¸­å¸¸è§ç”¨ç¬¬ä¸‰äººç§°ç§°å‘¼è‡ªå·±çš„å¯¹è¯ï¼Œå¿…é¡»æ”¹æˆç¬¬ä¸€äººç§°ä»¥ç¬¦åˆä¸­æ–‡é˜…è¯»ä¹ æƒ¯ã€‚ 
            â€¢ **æ ‡ç‚¹è§„èŒƒ**ï¼šç”¨ä¸­æ–‡æ ‡ç‚¹ï¼Œè‹±æ–‡ä¸“åå†…éƒ¨ä¿ç•™åŠè§’ã€‚ç¦æ­¢å‡ºç°å¤šä¸ªè¿ç»­å¥å·ï¼ˆå¦‚ã€‚ã€‚ã€‚ã€.ã€‚ã€‚ã€‚ã€.ã€‚.ç­‰ï¼‰ï¼Œç»Ÿä¸€ä½¿ç”¨çœç•¥å·â€¦â€¦ã€‚  
            â€¢ **è¯æ±‡é€‰æ‹©**ï¼šä¼˜å…ˆé€‰æ‹©å¯Œæœ‰æ–‡å­¦è‰²å½©çš„è¯æ±‡ï¼Œé¿å…è¿‡äºå£è¯­åŒ–æˆ–ç”Ÿç¡¬çš„è¡¨è¾¾ï¼Œè¿½æ±‚é›…ä¿—å…±èµçš„è¯­è¨€é£æ ¼ã€‚
            â€¢ **å¥å¼å˜åŒ–**ï¼šçµæ´»è¿ç”¨é•¿çŸ­å¥ç»“åˆï¼Œé¿å…å¥å¼å•è°ƒï¼Œè¥é€ ä¸°å¯Œçš„è¯­è¨€èŠ‚å¥ã€‚
            â€¢ æ•°å­—ã€è®¡é‡å•ä½ã€è´§å¸ç¬¦å·ç…§åŸæ–‡ã€‚ 
            â€¢ ä¿æŒåŸæ–‡çš„å™äº‹èŠ‚å¥ã€è¯­è°ƒå’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹å¼ï¼ŒåŒæ—¶æå‡ä¸­æ–‡è¡¨è¾¾çš„æ–‡å­¦æ€§ã€‚

            ===== è¾“å‡ºæ ¼å¼ç¤ºä¾‹ ===== 
            è¾“å…¥ï¼š 
            <c1>Page 1 of 506</c1> 
            <c2>Khun Somchai said to P'Niran</c2> 
            <c3>Chapter 1: The Beginning</c3> 
            <c4>â˜˜ 02.Love Story â˜˜</c4>
            <c5>"I love you," John whispered to Mary.</c5>

            è¾“å‡ºï¼š 
            <c1>[é¡µçœ‰é¡µè„š]</c1> 
            <c2>Khun Somchaiå¯¹P'Niranè¯´</c2> 
            <c3>## ç¬¬01ç«  å¼€å§‹</c3> 
            <c4>## ç¬¬02ç«  Love Story</c4>
            <c5>"æˆ‘çˆ±ä½ ï¼Œ"Johnå¯¹Maryè½»å£°è¯´é“ã€‚</c5>

            ===== ä¸¥æ ¼éµå®ˆè¾“å‡ºæ ¼å¼ ===== 
            <c1>ç¬¬ä¸€æ®µè¯‘æ–‡æˆ–ç©º</c1> 
            <c2>ç¬¬äºŒæ®µè¯‘æ–‡æˆ–ç©º</c2> 
            ...
            """.strip())

        # --- è°ƒç”¨ LLM ---
        try:
            logging.info(f"ğŸ¤– å¼€å§‹ç¿»è¯‘æ‰¹æ¬¡{batch_num}ï¼Œå†…å®¹é•¿åº¦: {len(tagged_eng)} å­—ç¬¦")
            llm_out = call_llm(system_prompt, tagged_eng)
            
            if not llm_out or not llm_out.strip():
                raise ValueError("LLMè¿”å›å†…å®¹ä¸ºç©º")
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡{batch_num}ç¿»è¯‘å¤±è´¥: {e}")
            # åˆ›å»ºé”™è¯¯å ä½ç¬¦
            MISSING_DICT[batch_id] = ["ç¿»è¯‘å¤±è´¥"]
            error_content = f"**ç¿»è¯‘å¤±è´¥**: {e}\n\nåŸæ–‡:\n{raw_eng[:500]}...\n"
            
            batch_path = CHAP_DIR / f"{batch_id}.md"
            batch_path.write_text(error_content, encoding="utf-8")
            big_md_parts.append(error_content)
            
            logging.warning(f"æ‰¹æ¬¡{batch_num}å·²ä¿å­˜é”™è¯¯å ä½ç¬¦")
            continue

        # --- æ¸…æ´— & è§£æ ---
        try:
            cn_body, new_terms_block, miss = strip_tags(llm_out, keep_missing=True)
            MISSING_DICT[batch_id] = miss
            
            # éªŒè¯ç¿»è¯‘è´¨é‡
            if not cn_body.strip():
                raise ValueError("ç¿»è¯‘ç»“æœä¸ºç©º")
            
            # å¼ºåŒ–å®Œæ•´æ€§æ£€æŸ¥
            original_segments = len(re.findall(r'<c\d+>', tagged_eng))
            # è®¡ç®—æœ€ç»ˆè¯‘æ–‡çš„æ®µè½æ•°ï¼ˆåŸºäºæ¸…æ´—åçš„å†…å®¹ï¼‰
            translated_segments = len([p for p in cn_body.split('\n\n') if p.strip()])
            
            # æ£€æŸ¥æ¯ä¸ªè¾“å…¥æ®µè½æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„è¾“å‡º
            input_tags = set(re.findall(r'<c(\d+)>', tagged_eng))
            output_tags = set(re.findall(r'<c(\d+)>', llm_out))
            missing_tags = input_tags - output_tags
            
            # æ‰“å°è¯¦ç»†çš„æ®µè½å¯¹æ¯”ä¿¡æ¯
            logging.info(f"ğŸ“Š æ‰¹æ¬¡{batch_num}æ®µè½æ•°é‡å¯¹æ¯”: è¾“å…¥{original_segments}æ®µ â†’ è¾“å‡º{translated_segments}æ®µ")
            
            if missing_tags:
                missing_list = sorted([int(tag) for tag in missing_tags])
                logging.error(f"ğŸš¨ æ‰¹æ¬¡{batch_num}å‘ç°é—æ¼æ®µè½: c{missing_list}")
                # å°†é—æ¼çš„æ®µè½æ·»åŠ åˆ°ç¼ºå¤±åˆ—è¡¨
                for tag_num in missing_list:
                    if f"c{tag_num:03d}" not in miss:
                        miss.append(f"c{tag_num:03d}")
                WARNING_DICT[batch_id] = f"é—æ¼æ®µè½: c{missing_list}"
            elif abs(original_segments - translated_segments) > original_segments * 0.2:  # å…è®¸20%çš„å·®å¼‚
                warning_msg = f"åŸæ–‡{original_segments}æ®µ vs è¯‘æ–‡{translated_segments}æ®µ"
                logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æ®µè½æ•°é‡å·®å¼‚è¾ƒå¤§: {warning_msg}")
                WARNING_DICT[batch_id] = warning_msg
            elif original_segments != translated_segments:
                # æ‰“å°æ‰€æœ‰æ®µè½æ•°é‡ä¸ä¸€è‡´çš„æƒ…å†µï¼Œå³ä½¿å·®å¼‚åœ¨å…è®¸èŒƒå›´å†…
                diff_msg = f"åŸæ–‡{original_segments}æ®µ vs è¯‘æ–‡{translated_segments}æ®µ"
                logging.info(f"â„¹ï¸  æ‰¹æ¬¡{batch_num}æ®µè½æ•°é‡ä¸ä¸€è‡´: {diff_msg}")
            else:
                logging.info(f"âœ… æ‰¹æ¬¡{batch_num}æ®µè½å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡{batch_num}ç»“æœè§£æå¤±è´¥: {e}")
            MISSING_DICT[batch_id] = ["è§£æå¤±è´¥"]
            cn_body = f"**è§£æå¤±è´¥**: {e}\n\nåŸå§‹LLMè¾“å‡º:\n{llm_out[:1000]}..."
            new_terms_block = ""

        # --- æœ¯è¯­è¡¨å¤„ç†å·²ç§»é™¤ï¼Œä¸å†å¤„ç†æœ¯è¯­è¡¨ç›¸å…³å†…å®¹ ---

        # --- å†™æ‰¹æ¬¡æ–‡ä»¶ ---
        try:
            batch_path = CHAP_DIR / f"{batch_id}.md"
            batch_content = f"{cn_body}\n"
            batch_path.write_text(batch_content, encoding="utf-8")
            big_md_parts.append(batch_content)
            
            # éªŒè¯æ–‡ä»¶å†™å…¥
            if not batch_path.exists() or batch_path.stat().st_size == 0:
                raise IOError("æ–‡ä»¶å†™å…¥å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º")
            
            logging.info(f"âœ… æ‰¹æ¬¡ {batch_num} ç¿»è¯‘å®Œæˆ â†’ {batch_path.name}")
            if miss:
                logging.warning(f"âš ï¸  æ‰¹æ¬¡ {batch_num} æœ‰ {len(miss)} ä¸ªç¼ºå¤±æ®µè½")
            
        except Exception as e:
            logging.error(f"âŒ æ‰¹æ¬¡{batch_num}æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
            raise
        
        # æœ¯è¯­è¡¨ä¿å­˜åŠŸèƒ½å·²ç§»é™¤
        
        # é€‚åº¦é™é€Ÿ
        time.sleep(1)
        
    except Exception as e:
        logging.error(f"å¤„ç†æ‰¹æ¬¡{batch_num}æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
        MISSING_DICT[batch_id] = [f"å¤„ç†é”™è¯¯: {str(e)}"]
        continue

# ========= æ±‡æ€»è¾“å‡º ========= #
logging.info("=== å¼€å§‹ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š ===")

# ç»Ÿè®¡ä¿¡æ¯
processed_batches_success = len([bid for bid in MISSING_DICT if not any("å¤„ç†é”™è¯¯" in str(m) for m in MISSING_DICT[bid])])  # æ’é™¤ä¸¥é‡é”™è¯¯çš„æ‰¹æ¬¡
failed_batches = [bid for bid, miss_list in MISSING_DICT.items() if any("å¤±è´¥" in str(m) or "é”™è¯¯" in str(m) for m in miss_list)]
missing_segments = sum(len([m for m in miss_list if m != "ç¿»è¯‘å¤±è´¥" and "é”™è¯¯" not in str(m)]) for miss_list in MISSING_DICT.values())

# 1. æœ¯è¯­è¡¨åŠŸèƒ½å·²ç§»é™¤
logging.info("æœ¯è¯­è¡¨åŠŸèƒ½å·²ç¦ç”¨ï¼Œä¸“æœ‰åè¯é€šè¿‡promptçº¦æŸå¤„ç†")

# 2. ç¼ºå¤±æ®µè½æ¸…å•
try:
    missing_report = [
        "# ç¿»è¯‘è´¨é‡æŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"æ€»æ‰¹æ¬¡æ•°: {total_batches}",
        f"æˆåŠŸå¤„ç†: {processed_batches_success}",
        f"å¤±è´¥æ‰¹æ¬¡: {len(failed_batches)}",
        f"ç¼ºå¤±æ®µè½: {missing_segments}",
        f"æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH}",
        "",
        "## è¯¦ç»†ä¿¡æ¯"
    ]
    
    # å¤±è´¥æ‰¹æ¬¡
    if failed_batches:
        missing_report.extend(["", "### å¤±è´¥æ‰¹æ¬¡"])
        for bid in sorted(failed_batches):
            missing_report.append(f"- {bid}: {', '.join(MISSING_DICT[bid])}")
    
    # ç¼ºå¤±æ®µè½
    batches_with_missing = {bid: miss_list for bid, miss_list in MISSING_DICT.items() 
                           if miss_list and not any("å¤±è´¥" in str(m) or "é”™è¯¯" in str(m) for m in miss_list)}
    
    if batches_with_missing:
        missing_report.extend(["", "### ç¼ºå¤±æ®µè½"])
        for bid in sorted(batches_with_missing):
            if batches_with_missing[bid]:
                missing_report.append(f"- {bid}: {', '.join(batches_with_missing[bid])}")
    
    # æ®µè½æ•°é‡å·®å¼‚è­¦å‘Š
    if WARNING_DICT:
        missing_report.extend(["", "### æ®µè½æ•°é‡å·®å¼‚è­¦å‘Š"])
        for bid in sorted(WARNING_DICT):
            missing_report.append(f"- {bid}: {WARNING_DICT[bid]}")
    
    # æˆåŠŸæ‰¹æ¬¡
    successful_batches = [bid for bid in MISSING_DICT if bid not in failed_batches and not MISSING_DICT[bid]]
    if successful_batches:
        missing_report.extend(["", "### å®Œå…¨æˆåŠŸæ‰¹æ¬¡"])
        missing_report.append(f"å…±{len(successful_batches)}æ‰¹æ¬¡: {', '.join(sorted(successful_batches))}")
    
    missing_path = OUT_DIR / "translation_report.txt"
    missing_path.write_text("\n".join(missing_report), encoding="utf-8")
    logging.info(f"ç¿»è¯‘æŠ¥å‘Šå·²ä¿å­˜ â†’ {missing_path}")
except Exception as e:
    logging.error(f"ç¿»è¯‘æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

# 3. åˆå¹¶Markdown
try:
    if big_md_parts:
        # æ·»åŠ æ–‡æ¡£å¤´éƒ¨
        header = f"""å…¨æ–‡æœºç¿»  
æ›´å¤šæ³°ç™¾å°è¯´è§ thaigl.drifting.boats

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

"""
        
        big_md_content = header + "\n".join(big_md_parts)
        big_md_path = OUT_DIR / BIG_MD_NAME
        big_md_path.write_text(big_md_content, encoding="utf-8")
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        file_size = big_md_path.stat().st_size
        logging.info(f"å…¨é›† Markdown æ±‡æ€»å®Œæˆ â†’ {big_md_path} ({file_size:,} å­—èŠ‚)")
    else:
        logging.warning("æ²¡æœ‰æˆåŠŸç¿»è¯‘çš„æ‰¹æ¬¡ï¼Œè·³è¿‡Markdownæ±‡æ€»")
except Exception as e:
    logging.error(f"Markdownæ±‡æ€»å¤±è´¥: {e}")

# 4. æœ€ç»ˆç»Ÿè®¡
logging.info("=== ç¿»è¯‘æµç¨‹å®Œæˆ ===")
log_progress(total_batches, total_batches, "æœ€ç»ˆè¿›åº¦", "å®Œæˆ")
logging.info(f"âœ… å¤„ç†ç»“æœ: {processed_batches_success}/{total_batches} æ‰¹æ¬¡æˆåŠŸ")
if failed_batches:
    logging.warning(f"âŒ å¤±è´¥æ‰¹æ¬¡: {', '.join(failed_batches)}")
if missing_segments > 0:
    logging.warning(f"âš ï¸  æ€»è®¡ç¼ºå¤±æ®µè½: {missing_segments}")
else:
    logging.info("ğŸ‰ æ‰€æœ‰æ®µè½ç¿»è¯‘å®Œæˆï¼")

# 5. æˆæœ¬ç»Ÿè®¡æ€»ç»“
if CONFIG.get('pricing', {}).get('enable_cost_tracking', False):
    pricing = CONFIG.get('pricing', {})
    currency = pricing.get('currency', 'USD')
    logging.info("=== æˆæœ¬ç»Ÿè®¡æ€»ç»“ ===")
    logging.info(f"ğŸ“Š æ€»Tokenä½¿ç”¨: è¾“å…¥{total_input_tokens:,} + è¾“å‡º{total_output_tokens:,} = æ€»è®¡{total_input_tokens + total_output_tokens:,}")
    logging.info(f"ğŸ’° æ€»æˆæœ¬: {total_cost:.4f} {currency}")
    if total_input_tokens > 0:
        avg_cost_per_1k_input = (total_cost * 1000) / (total_input_tokens + total_output_tokens) if (total_input_tokens + total_output_tokens) > 0 else 0
        logging.info(f"ğŸ“ˆ å¹³å‡æˆæœ¬: {avg_cost_per_1k_input:.4f} {currency}/1K tokens")
else:
    logging.info("ğŸ“Š Tokenç»Ÿè®¡: è¾“å…¥{:,} + è¾“å‡º{:,} = æ€»è®¡{:,}".format(total_input_tokens, total_output_tokens, total_input_tokens + total_output_tokens))

# 6. ç”Ÿæˆé‡è¯•è„šæœ¬ï¼ˆå¦‚æœæœ‰å¤±è´¥æ‰¹æ¬¡ï¼‰
if failed_batches:
    try:
        retry_config = CONFIG.copy()
        # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡ç”Ÿæˆé‡è¯•é…ç½®
        retry_batches = []
        for bid in failed_batches:
            if bid.startswith("batch_"):
                batch_num = int(bid.split("_")[1])
                p_start = (batch_num - 1) * PAGES_PER_BATCH + 1
                p_end = min(batch_num * PAGES_PER_BATCH, total_pages)
                retry_batches.append({"batch_num": batch_num, "pages": [p_start, p_end]})
        
        retry_config["failed_batches"] = retry_batches
        retry_config["pages_per_batch"] = PAGES_PER_BATCH
        
        retry_config_path = OUT_DIR / "retry_config.json"
        with open(retry_config_path, 'w', encoding='utf-8') as f:
            json.dump(retry_config, f, ensure_ascii=False, indent=2)
        
        logging.info(f"é‡è¯•é…ç½®å·²ç”Ÿæˆ â†’ {retry_config_path}")
        logging.info("å¯ä½¿ç”¨æ­¤é…ç½®é‡æ–°è¿è¡Œè„šæœ¬å¤„ç†å¤±è´¥æ‰¹æ¬¡")
    except Exception as e:
        logging.warning(f"é‡è¯•é…ç½®ç”Ÿæˆå¤±è´¥: {e}")

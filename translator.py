#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
page_batch_translation_agent_cn.py
â€”â€” æŒ‰é¡µæ•°åˆ†æ‰¹ PDF è‹±è¯‘æœ¬ â†’ ä¸­æ–‡è¯‘æœ¬æ‰¹é‡ç¿»è¯‘è„šæœ¬
  â€¢ æŒ‰æŒ‡å®šé¡µæ•°Xè‡ªåŠ¨åˆ†æ‰¹ç¿»è¯‘
  â€¢ å¤„ç†å¥å­å®Œæ•´æ€§ï¼Œç¡®ä¿ç¿»è¯‘è‡³å¥å­ç»“æŸ
  â€¢ è‡ªåŠ¨è¯†åˆ«æ ‡é¢˜å¹¶ä¿æŒæ ¼å¼
  â€¢ è‡ªåŠ¨ç¼–å·å¹¶æœ€ç»ˆæ•´åˆä¸ºä¸€ä¸ªæ–‡ä»¶
  â€¢ å¢é‡æ›´æ–°æœ¯è¯­è¡¨ glossary.tsv
"""
import json, re, textwrap, logging, time, datetime, requests, os, sys, warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# è¿‡æ»¤pdfminerçš„å­—ä½“è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning, module="pdfminer")

from pdfminer.high_level import extract_text   # pip install pdfminer.six

# ========= ç¼“å­˜ç®¡ç†åŠŸèƒ½ ========= #
def clean_cache_files(cache_dir: Path, pdf_path: Path = None, force: bool = False):
    """æ¸…ç†ç¼“å­˜æ–‡ä»¶
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•
        pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
    """
    if not cache_dir.exists():
        return
    
    cache_patterns = [
        "*_text_cache.txt",      # PDFæ–‡æœ¬ç¼“å­˜
        "batch_*_raw_text.txt",  # æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    ]
    
    cleaned_count = 0
    total_size_cleaned = 0
    
    logging.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ç¼“å­˜æ–‡ä»¶ (å¼ºåˆ¶æ¸…ç†: {'æ˜¯' if force else 'å¦'})")
    
    for pattern in cache_patterns:
        for cache_file in cache_dir.glob(pattern):
            should_clean = force
            
            if not should_clean and pdf_path and pdf_path.exists():
                try:
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    pdf_mtime = pdf_path.stat().st_mtime
                    cache_mtime = cache_file.stat().st_mtime
                    should_clean = cache_mtime < pdf_mtime
                except Exception:
                    should_clean = True  # å‡ºé”™æ—¶æ¸…ç†
            
            if should_clean:
                try:
                    file_size = cache_file.stat().st_size
                    cache_file.unlink()
                    cleaned_count += 1
                    total_size_cleaned += file_size
                    logging.info(f"ğŸ—‘ï¸  åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file.name} ({file_size/1024:.1f}KB)")
                except Exception as e:
                    logging.warning(f"âš ï¸  æ¸…ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
    
    if cleaned_count > 0:
        logging.info(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_size_cleaned/1024:.1f}KB ç©ºé—´")
    else:
        logging.info("ğŸ’¾ æ— éœ€æ¸…ç†ç¼“å­˜æ–‡ä»¶")

# ========= è¯»å–é…ç½® ========= #
def load_config(config_file: str = None) -> Dict:
    """å®‰å…¨åŠ è½½é…ç½®æ–‡ä»¶"""
    ROOT = Path(__file__).resolve().parent
    
    if config_file:
        config_path = Path(config_file)
        if not config_path.is_absolute():
            config_path = ROOT / config_file
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œå°è¯•ä»å‘½ä»¤è¡Œå‚æ•°è·å–
        if len(sys.argv) > 1:
            config_path = Path(sys.argv[1])
            if not config_path.is_absolute():
                config_path = ROOT / sys.argv[1]
        else:
            # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„config.json
            config_path = ROOT / "config.json"
    
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    try:
        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)
        
        # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        required_keys = ["api", "paths"]
        for key in required_keys:
            if key not in config:
                raise KeyError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€é¡¹: {key}")
        
        # éªŒè¯APIé…ç½®
        api_keys = ["API_URL", "API_KEY", "LLM_MODEL"]
        for key in api_keys:
            if key not in config["api"]:
                raise KeyError(f"APIé…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
        
        # éªŒè¯è·¯å¾„é…ç½®
        path_keys = ["pdf", "output_dir", "big_md_name"]
        for key in path_keys:
            if key not in config["paths"]:
                raise KeyError(f"è·¯å¾„é…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
        
        return config
    except json.JSONDecodeError as e:
        raise ValueError(f"é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

ROOT = Path(__file__).resolve().parent

# äº¤äº’å¼è·å–é…ç½®æ–‡ä»¶è·¯å¾„
def get_config_path() -> str:
    """äº¤äº’å¼è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    if len(sys.argv) > 1:
        return sys.argv[1]
    
    print("è¯·é€‰æ‹©é…ç½®æ–‡ä»¶:")
    print("1. ä½¿ç”¨é»˜è®¤é…ç½® (config.json)")
    print("2. è¾“å…¥è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„")
    
    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
        if choice == "1":
            return "config.json"
        elif choice == "2":
            config_path = input("è¯·è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„: ").strip()
            if config_path:
                return config_path
            else:
                print("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")

config_file = get_config_path()
CONFIG: Dict = load_config(config_file)
print(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}")

API_URL      = CONFIG["api"]["API_URL"]
API_KEY      = CONFIG["api"]["API_KEY"]
LLM_MODEL    = CONFIG["api"]["LLM_MODEL"]
TEMPERATURE  = CONFIG["api"].get("temperature", 0.2)

PDF_PATH     = Path(CONFIG["paths"]["pdf"])
OUT_DIR      = Path(CONFIG["paths"]["output_dir"])
BIG_MD_NAME  = CONFIG["paths"]["big_md_name"]

# æ–°å¢é…ç½®é¡¹ï¼šæ¯æ‰¹å¤„ç†çš„é¡µæ•°
PAGES_PER_BATCH = CONFIG.get("pages_per_batch", 10)  # é»˜è®¤æ¯10é¡µç¿»è¯‘ä¸€æ¬¡
GLOSS_CFG    = CONFIG.get("glossary", {})

# éªŒè¯PDFæ–‡ä»¶å­˜åœ¨
if not PDF_PATH.exists():
    raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {PDF_PATH}")

# ========= ç¼“å­˜ç®¡ç† ========= #

# åˆå§‹åŒ–style_cacheç›¸å…³
STYLE_FILE = OUT_DIR / "style_cache.txt"
style_cache = ""
if STYLE_FILE.exists():
    try:
        style_cache = STYLE_FILE.read_text(encoding="utf-8").strip()
    except Exception as e:
        logging.warning(f"è¯»å–é£æ ¼ç¼“å­˜å¤±è´¥: {e}")
        style_cache = ""

# ========= å¸¸é‡ ========= #
HEAD_SEP  = "\n" + ("â”€"*80) + "\n"
TAG_PAT   = re.compile(r"<c\d+>(.*?)</c\d+>", re.S)
NEWTERM_PAT = re.compile(r"```glossary(.*?)```", re.S)

# ========= æ—¥å¿—é…ç½® ========= #
class ColoredFormatter(logging.Formatter):
    """å½©è‰²æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    # é¢œè‰²ä»£ç 
    COLORS = {
        'DEBUG': '\033[36m',    # é’è‰²
        'INFO': '\033[32m',     # ç»¿è‰²
        'WARNING': '\033[33m',  # é»„è‰²
        'ERROR': '\033[31m',    # çº¢è‰²
        'CRITICAL': '\033[35m', # ç´«è‰²
        'RESET': '\033[0m'      # é‡ç½®
    }
    
    def format(self, record):
        # æ·»åŠ é¢œè‰²
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        reset = self.COLORS['RESET']
        
        # æ ¼å¼åŒ–æ—¶é—´
        record.asctime = self.formatTime(record, self.datefmt)
        
        # æ ¹æ®æ—¥å¿—çº§åˆ«ä½¿ç”¨ä¸åŒæ ¼å¼
        if record.levelname == 'INFO':
            if '===' in record.getMessage():
                # æ‰¹æ¬¡å¤„ç†æ ‡é¢˜
                return f"{color}{'='*60}{reset}\n{color}[{record.asctime}] {record.getMessage()}{reset}\n{color}{'='*60}{reset}"
            elif 'è¿›åº¦:' in record.getMessage():
                # è¿›åº¦ä¿¡æ¯
                return f"{color}[{record.asctime}] ğŸ“Š {record.getMessage()}{reset}"
            elif 'ç¼“å­˜' in record.getMessage():
                # ç¼“å­˜ç›¸å…³
                return f"{color}[{record.asctime}] ğŸ’¾ {record.getMessage()}{reset}"
            elif 'å®Œæˆ' in record.getMessage() or 'æˆåŠŸ' in record.getMessage():
                # æˆåŠŸä¿¡æ¯
                return f"{color}[{record.asctime}] âœ… {record.getMessage()}{reset}"
            else:
                return f"{color}[{record.asctime}] â„¹ï¸  {record.getMessage()}{reset}"
        elif record.levelname == 'WARNING':
            return f"{color}[{record.asctime}] âš ï¸  {record.getMessage()}{reset}"
        elif record.levelname == 'ERROR':
            return f"{color}[{record.asctime}] âŒ {record.getMessage()}{reset}"
        else:
            return f"{color}[{record.asctime}] [{record.levelname}] {record.getMessage()}{reset}"

def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = logging.DEBUG if verbose else logging.INFO
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(ColoredFormatter(datefmt="%H:%M:%S"))
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=level,
        handlers=[console_handler],
        force=True
    )

def log_progress(current: int, total: int, prefix: str = "è¿›åº¦", suffix: str = ""):
    """æ˜¾ç¤ºè¿›åº¦æ¡"""
    percent = (current / total) * 100
    bar_length = 30
    filled_length = int(bar_length * current // total)
    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
    
    logging.info(f"{prefix}: [{bar}] {percent:.1f}% ({current}/{total}) {suffix}")

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
setup_logging(verbose=CONFIG.get('verbose_logging', False))

# ========= è¾…åŠ©å‡½æ•° ========= #
# ç§»é™¤äº†detect_titleså‡½æ•°ï¼Œç°åœ¨ç”±LLMè´Ÿè´£è¯†åˆ«æ ‡é¢˜å’Œé¡µçœ‰é¡µç 

def ensure_sentence_completion(text: str) -> str:
    """ç¡®ä¿æ–‡æœ¬ä»¥å®Œæ•´å¥å­ç»“æŸ"""
    text = text.strip()
    if not text:
        return text
    
    # æ£€æŸ¥æ˜¯å¦ä»¥å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç»“å°¾
    if text[-1] in '.!?':
        return text
    
    # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´å¥å­çš„ç»“æŸä½ç½®
    last_sentence_end = -1
    for i in range(len(text) - 1, -1, -1):
        if text[i] in '.!?':
            # ç¡®ä¿ä¸æ˜¯ç¼©å†™ï¼ˆå¦‚Mr. Dr.ç­‰ï¼‰
            if i < len(text) - 1 and text[i+1] in ' \n\t':
                last_sentence_end = i
                break
            elif i == len(text) - 1:
                last_sentence_end = i
                break
    
    if last_sentence_end > 0:
        return text[:last_sentence_end + 1]
    
    return text

def wrap_batch_with_tags(raw_text: str) -> str:
    """æŠŠæ‰¹æ¬¡åŸæ–‡æŒ‰ç©ºè¡Œåˆ†æ®µï¼ŒåŠ  <c1>â€¦</c1> æ ‡ç­¾ï¼Œè®©LLMè‡ªè¡Œè¯†åˆ«æ ‡é¢˜å’Œé¡µçœ‰é¡µç """
    segments = [seg.strip() for seg in re.split(r"\n\s*\n", raw_text) if seg.strip()]
    tagged = []
    
    for idx, seg in enumerate(segments, start=1):
        # ä¸å†é¢„å…ˆæ ‡è®°æ ‡é¢˜ï¼Œè®©LLMè‡ªè¡Œè¯†åˆ«å’Œå¤„ç†
        tagged.append(f"<c{idx}>{seg}</c{idx}>")
    
    return "\n\n".join(tagged)

def strip_tags(llm_output: str, keep_missing: bool = True):
    """æ¸…æ´— LLM è¾“å‡º & æ”¶é›†ç¼ºå¤±æ®µ"""
    paragraphs = TAG_PAT.findall(llm_output)

    miss_list, clean_paras = [], []
    for idx, p in enumerate(paragraphs, start=1):
        if p.strip() == "{{MISSING}}":
            miss_list.append(f"c{idx:03d}")
            if keep_missing:
                clean_paras.append("{{MISSING}}")
        elif p.strip() == "" or p.strip().startswith("[é¡µçœ‰é¡µè„š]") or p.strip().startswith("[ç›®å½•]"):  # å¤„ç†ç©ºæ ‡ç­¾å’Œç‰¹æ®Šæ ‡è®°
            # è·³è¿‡ç©ºå†…å®¹å’Œé¡µçœ‰é¡µè„šã€ç›®å½•æ ‡è®°ï¼Œä¸æ·»åŠ åˆ°clean_parasä¸­
            pass
        else:
            clean_paras.append(p.strip())

    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…å¤šä½™çš„ç©ºè¡Œ
    clean_paras = [para for para in clean_paras if para.strip()]
    pure_text = "\n\n".join(clean_paras)
    new_terms_block = "\n".join(
        line.strip()
        for blk in NEWTERM_PAT.findall(llm_output)
        for line in blk.strip().splitlines() if line.strip()
    )
    return pure_text, new_terms_block, miss_list

def refresh_style(sample_text: str):
    """è‹¥ style_cache ä¸ºç©ºï¼Œåˆ™ç”¨åŸæ–‡æ ·æœ¬è®© LLM å½’çº³é£æ ¼ï¼›å¦åˆ™è·³è¿‡"""
    global style_cache
    if style_cache:
        return
    
    try:
        prompt_sys = "You are a literary critic. Analyze the writing style concisely."
        # ç¡®ä¿æ ·æœ¬æ–‡æœ¬ä¸è¶…è¿‡2000å­—ç¬¦ï¼Œé¿å…APIé™åˆ¶
        sample_text = sample_text[:2000] if len(sample_text) > 2000 else sample_text
        prompt_user = f"""Summarize the narrative voice, tone, humor level and sentence rhythm of the following English text in 80 words:

{sample_text}"""
        
        style_cache = call_llm(prompt_sys, prompt_user)
        if style_cache:
            STYLE_FILE.write_text(style_cache, encoding="utf-8")
            logging.info("é£æ ¼åˆ†æå®Œæˆå¹¶å·²ç¼“å­˜")
        else:
            raise ValueError("é£æ ¼åˆ†æè¿”å›ç©ºç»“æœ")
    except Exception as e:
        logging.error(f"é£æ ¼åˆ†æå¤±è´¥: {e}")
        style_cache = "é»˜è®¤é£æ ¼ï¼šä¿æŒåŸæ–‡çš„å™äº‹èŠ‚å¥å’Œè¯­è°ƒ"

def load_glossary(path: Path) -> Dict[str, str]:
    if not path.exists():
        return {}
    out = {}
    for line in path.read_text(encoding="utf-8").splitlines():
        if "\t" in line:
            src, tgt = line.split("\t", 1)
            out[src.strip()] = tgt.strip()
    return out

def save_glossary(gls: Dict[str,str], path: Path):
    lines = [f"{k}\t{v}" for k, v in sorted(gls.items())]
    path.write_text("\n".join(lines), encoding="utf-8")

def call_llm(prompt_sys: str, prompt_user: str, max_retries: int = 3, timeout: int = 120) -> str:
    """è°ƒç”¨LLM APIï¼Œå¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": LLM_MODEL,
        "temperature": TEMPERATURE,
        "messages": [
            {"role": "system", "content": prompt_sys},
            {"role": "user", "content": prompt_user}
        ]
    }
    
    last_error = None
    for attempt in range(max_retries):
        try:
            logging.info(f"ğŸ¤– è°ƒç”¨LLM API - æ¨¡å‹: {LLM_MODEL} (å°è¯• {attempt+1}/{max_retries})")
            logging.debug(f"ç³»ç»Ÿæç¤ºé•¿åº¦: {len(prompt_sys)} å­—ç¬¦")
            logging.debug(f"ç”¨æˆ·è¾“å…¥é•¿åº¦: {len(prompt_user)} å­—ç¬¦")
            
            resp = requests.post(API_URL, headers=headers, json=payload, timeout=timeout)
            resp.raise_for_status()
            
            result = resp.json()
            if "choices" not in result or not result["choices"]:
                raise ValueError("APIè¿”å›æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ")
            
            content = result["choices"][0]["message"]["content"]
            if not content or not content.strip():
                raise ValueError("APIè¿”å›å†…å®¹ä¸ºç©º")
            
            logging.info(f"âœ… LLMå“åº”æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è®°å½•tokenä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if "usage" in result and result["usage"]:
                usage = result["usage"]
                logging.info(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥{usage.get('prompt_tokens', 0)} + è¾“å‡º{usage.get('completion_tokens', 0)} = æ€»è®¡{usage.get('total_tokens', 0)}")
            
            return content.strip()
            
        except requests.exceptions.Timeout:
            last_error = f"è¯·æ±‚è¶…æ—¶({timeout}ç§’)"
            logging.warning(f"LLMè°ƒç”¨è¶…æ—¶({attempt+1}/{max_retries})")
        except requests.exceptions.ConnectionError:
            last_error = "ç½‘ç»œè¿æ¥é”™è¯¯"
            logging.warning(f"LLMè°ƒç”¨ç½‘ç»œé”™è¯¯({attempt+1}/{max_retries})")
        except requests.exceptions.HTTPError as e:
            last_error = f"HTTPé”™è¯¯: {e.response.status_code}"
            logging.warning(f"LLMè°ƒç”¨HTTPé”™è¯¯({attempt+1}/{max_retries}): {e}")
        except (KeyError, ValueError, json.JSONDecodeError) as e:
            last_error = f"å“åº”è§£æé”™è¯¯: {e}"
            logging.warning(f"LLMè°ƒç”¨è§£æé”™è¯¯({attempt+1}/{max_retries}): {e}")
        except Exception as e:
            last_error = f"æœªçŸ¥é”™è¯¯: {e}"
            logging.warning(f"LLMè°ƒç”¨æœªçŸ¥é”™è¯¯({attempt+1}/{max_retries}): {e}")
        
        if attempt < max_retries - 1:
            wait_time = min(5 * (2 ** attempt), 30)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§30ç§’
            logging.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    raise RuntimeError(f"LLMè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}")

# ========= å‡†å¤‡è¾“å‡ºç›®å½• ========= #
try:
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    CHAP_DIR = OUT_DIR / "chap_md"
    CHAP_DIR.mkdir(exist_ok=True)
    RAW_CONTENT_DIR = OUT_DIR / "raw_content"
    RAW_CONTENT_DIR.mkdir(exist_ok=True)
    logging.info(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {OUT_DIR}")
    logging.info(f"åŸå§‹å†…å®¹ç›®å½•å·²å‡†å¤‡: {RAW_CONTENT_DIR}")
except Exception as e:
    raise RuntimeError(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")

# æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†è¿‡æœŸç¼“å­˜
if CONFIG.get("clean_cache_on_start", True):
    logging.info("=== æ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶ ===")
    clean_cache_files(RAW_CONTENT_DIR, PDF_PATH, force=False)

gloss_path = OUT_DIR / "glossary.tsv"
GLOSSARY = load_glossary(gloss_path)
# é¢„ç½®ï¼šconfig ä¸­å£°æ˜çš„è¯æ±‡ï¼Œä¼˜å…ˆçº§æœ€é«˜
GLOSSARY.update(GLOSS_CFG)
logging.info(f"æœ¯è¯­è¡¨å·²åŠ è½½ï¼Œå…±{len(GLOSSARY)}ä¸ªæ¡ç›®")

# ========= è§£ææ•´æœ¬ PDFï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰========= #
def get_pdf_text_with_cache(pdf_path: Path, cache_dir: Path) -> str:
    """è·å–PDFæ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    # ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
    pdf_name = pdf_path.stem
    cache_file = cache_dir / f"{pdf_name}_text_cache.txt"
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    if cache_file.exists():
        try:
            pdf_mtime = pdf_path.stat().st_mtime
            cache_mtime = cache_file.stat().st_mtime
            
            # å¦‚æœç¼“å­˜æ–‡ä»¶æ¯”PDFæ–‡ä»¶æ–°ï¼Œä½¿ç”¨ç¼“å­˜
            if cache_mtime >= pdf_mtime:
                cache_size = cache_file.stat().st_size
                logging.info(f"ğŸ’¾ ä½¿ç”¨PDFæ–‡æœ¬ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB)")
                cached_text = cache_file.read_text(encoding="utf-8")
                logging.info(f"ğŸ“„ ç¼“å­˜æ–‡æœ¬é•¿åº¦: {len(cached_text)} å­—ç¬¦")
                return cached_text
            else:
                logging.info("ğŸ“ PDFæ–‡ä»¶å·²æ›´æ–°ï¼Œé‡æ–°æå–æ–‡æœ¬")
        except Exception as e:
            logging.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}ï¼Œå°†é‡æ–°æå–PDFæ–‡æœ¬")
    
    # æå–PDFæ–‡æœ¬
    logging.info(f"ğŸ” å¼€å§‹æå–PDFæ–‡æœ¬: {pdf_path.name}")
    import time
    start_time = time.time()
    
    full_text = extract_text(str(pdf_path), page_numbers=None)
    
    extract_time = time.time() - start_time
    
    # ä¿å­˜åˆ°ç¼“å­˜
    try:
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file.write_text(full_text, encoding="utf-8")
        cache_size = cache_file.stat().st_size
        logging.info(f"ğŸ’¾ PDFæ–‡æœ¬å·²ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB, è€—æ—¶{extract_time:.1f}ç§’)")
        logging.info(f"ğŸ“„ æå–æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    except Exception as e:
        logging.warning(f"ä¿å­˜æ–‡æœ¬ç¼“å­˜å¤±è´¥: {e}")
    
    return full_text

logging.info(f"å¼€å§‹åŠ è½½PDFæ–‡ä»¶: {PDF_PATH}")
try:
    # è·å–PDFæ–‡æœ¬ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    full_text = get_pdf_text_with_cache(PDF_PATH, RAW_CONTENT_DIR)
    if not full_text or not full_text.strip():
        raise ValueError("PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
    
    pages = re.split(r"\f", full_text)                           # pdfminer æŒ‰ formfeed åˆ†é¡µ
    if pages and pages[-1] == "":
        pages.pop()
    
    logging.info(f"PDFåŠ è½½å®Œæˆï¼Œå…±{len(pages)}é¡µ")
    
    # éªŒè¯é¡µæ•°é…ç½®
    max_page = len(pages)
    if PAGES_PER_BATCH < 1:
        raise ValueError(f"æ¯æ‰¹é¡µæ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰è®¾ç½®: {PAGES_PER_BATCH}")
    
    logging.info(f"å°†æŒ‰æ¯{PAGES_PER_BATCH}é¡µè¿›è¡Œåˆ†æ‰¹ç¿»è¯‘ï¼ŒPDFå…±{max_page}é¡µ")
    
except Exception as e:
    raise RuntimeError(f"PDFå¤„ç†å¤±è´¥: {e}")

# ========= æ±‡æ€»è¾“å‡º ========= #
MISSING_DICT = {}
WARNING_DICT = {}  # æ”¶é›†æ®µè½æ•°é‡å·®å¼‚ç­‰è­¦å‘Šä¿¡æ¯
big_md_parts = []
total_pages = len(pages)
total_batches = (total_pages + PAGES_PER_BATCH - 1) // PAGES_PER_BATCH  # å‘ä¸Šå–æ•´
processed_batches = 0

logging.info(f"=== å¼€å§‹åˆ†æ‰¹ç¿»è¯‘å¤„ç† ===")
logging.info(f"æ€»æ‰¹æ¬¡: {total_batches} | æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH} | æ€»é¡µæ•°: {total_pages}")

def get_batch_text_with_cache(pages: List[str], batch_num: int, p_start: int, p_end: int, cache_dir: Path) -> str:
    """è·å–æ‰¹æ¬¡æ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    batch_id = f"batch_{batch_num:03d}"
    cache_file = cache_dir / f"{batch_id}_raw_text.txt"
    
    # æ£€æŸ¥æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    if cache_file.exists():
        try:
            cached_text = cache_file.read_text(encoding="utf-8")
            if cached_text.strip():
                cache_size = cache_file.stat().st_size
                logging.debug(f"ğŸ’¾ ä½¿ç”¨æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB)")
                logging.debug(f"ğŸ“„ æ‰¹æ¬¡æ–‡æœ¬é•¿åº¦: {len(cached_text)} å­—ç¬¦")
                return cached_text
        except Exception as e:
            logging.warning(f"è¯»å–æ‰¹æ¬¡ç¼“å­˜å¤±è´¥: {e}")
    
    # ä»pagesæ•°ç»„ä¸­æå–æ–‡æœ¬
    raw_eng = "\n".join(pages[p_start-1:p_end])  # é¡µç ä» 1 å¼€å§‹
    
    # ä¿å­˜æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    try:
        cache_file.write_text(raw_eng, encoding="utf-8")
        cache_size = cache_file.stat().st_size
        logging.debug(f"ğŸ’¾ æ‰¹æ¬¡æ–‡æœ¬å·²ç¼“å­˜: {cache_file.name} ({cache_size/1024:.1f}KB)")
        logging.debug(f"ğŸ“„ æ‰¹æ¬¡æ–‡æœ¬é•¿åº¦: {len(raw_eng)} å­—ç¬¦")
    except Exception as e:
        logging.warning(f"ä¿å­˜æ‰¹æ¬¡ç¼“å­˜å¤±è´¥: {e}")
    
    return raw_eng

# æŒ‰é¡µæ•°åˆ†æ‰¹å¤„ç†
for batch_num in range(1, total_batches + 1):
    processed_batches += 1
    
    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„é¡µç èŒƒå›´
    p_start = (batch_num - 1) * PAGES_PER_BATCH + 1
    p_end = min(batch_num * PAGES_PER_BATCH, total_pages)
    batch_id = f"batch_{batch_num:03d}"
    
    logging.info(f"=== å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (é¡µ {p_start}-{p_end}) ===")
    log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", f"å½“å‰: æ‰¹æ¬¡{batch_num}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¿»è¯‘ç»“æœç¼“å­˜
    batch_md_file = CHAP_DIR / f"{batch_id}.md"
    if batch_md_file.exists():
        try:
            cached_content = batch_md_file.read_text(encoding="utf-8")
            if cached_content.strip():
                logging.info(f"ğŸ’¾ æ‰¹æ¬¡ {batch_num} å·²å­˜åœ¨ç¿»è¯‘ç»“æœï¼Œè·³è¿‡å¤„ç†")
                big_md_parts.append(cached_content)
                log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "ä½¿ç”¨ç¼“å­˜")
                continue
        except Exception as e:
            logging.warning(f"è¯»å–æ‰¹æ¬¡ç¿»è¯‘ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°å¤„ç†")
    
    try:
        # è·å–æ‰¹æ¬¡æ–‡æœ¬ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        raw_eng = get_batch_text_with_cache(pages, batch_num, p_start, p_end, RAW_CONTENT_DIR)
        
        if not raw_eng.strip():
            logging.warning(f"ğŸ“„ æ‰¹æ¬¡{batch_num}å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
            MISSING_DICT[batch_id] = ["æ•´æ‰¹ç¼ºå¤±"]
            log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "å†…å®¹ä¸ºç©º")
            continue
        
        # æ£€æŸ¥å¥å­å®Œæ•´æ€§ï¼Œå¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ä¸”å¥å­æœªå®Œæ•´ï¼Œå°è¯•æ‰©å±•
        if batch_num < total_batches:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•åˆ°ä¸‹ä¸€é¡µä»¥å®Œæˆå¥å­
            completed_text = ensure_sentence_completion(raw_eng)
            if len(completed_text) < len(raw_eng) * 0.8:  # å¦‚æœæˆªæ–­å¤ªå¤šï¼Œå°è¯•æ‰©å±•
                if p_end < total_pages:
                    # æ·»åŠ ä¸‹ä¸€é¡µçš„éƒ¨åˆ†å†…å®¹ç›´åˆ°å¥å­å®Œæ•´
                    next_page_text = pages[p_end] if p_end < len(pages) else ""
                    extended_text = raw_eng + "\n" + next_page_text
                    completed_extended = ensure_sentence_completion(extended_text)
                    if len(completed_extended) > len(completed_text):
                        raw_eng = completed_extended
                        logging.info(f"ğŸ“ æ‰¹æ¬¡{batch_num}æ‰©å±•åˆ°ä¸‹ä¸€é¡µä»¥å®Œæˆå¥å­")
            else:
                raw_eng = completed_text
        
        tagged_eng = wrap_batch_with_tags(raw_eng)
        
        # æ£€æŸ¥æ ‡ç­¾æ•°é‡æ˜¯å¦åˆç†
        tag_count = len(re.findall(r'<c\d+>', tagged_eng))
        if tag_count == 0:
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æœªèƒ½æ­£ç¡®åˆ†æ®µ")
        else:
            logging.info(f"ğŸ“ æ‰¹æ¬¡{batch_num}åˆ†ä¸º{tag_count}ä¸ªæ®µè½ï¼Œæ–‡æœ¬é•¿åº¦: {len(raw_eng)} å­—ç¬¦")

        # --- è·å–é£æ ¼ä¿¡æ¯ ---
        if not style_cache:
            # ä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‰å‡ æ®µä½œä¸ºé£æ ¼åˆ†ææ ·æœ¬
            sample_text = raw_eng[:5000]  # å–å‰5000å­—ç¬¦ä½œä¸ºæ ·æœ¬
            refresh_style(sample_text)
        
        # --- æ„é€ ç³»ç»Ÿæç¤º ---
        gloss_block = "\n".join(f"{k}\t{v}" for k,v in GLOSSARY.items())
        system_prompt = textwrap.dedent( f"""
            ä½ æ˜¯ä¸€å <èµ„æ·±æ–‡å­¦è¯‘è€…>ï¼Œéœ€æŠŠä¸‹æ–¹ã€æ³°è¯­â†’è‹±è¯‘â†’ä¸­æ–‡ã€‘çš„è‹±æ–‡å°è¯´ç²¾å‡†ã€é€å¥åœ°è¯‘æˆç°ä»£ä¸­æ–‡ã€‚

            ================ ä»»åŠ¡è¦æ±‚ ================
            1. **é€æ®µè½å¯¹é½**  
            â€¢ æºè‹±æ–‡ç”¨ <cN>â€¦</cN> æ ‡ç­¾åŒ…è£¹ï¼ˆè„šæœ¬å·²è‡ªåŠ¨åŠ ï¼‰ã€‚  
            â€¢ ä½ å¿…é¡»ä¸º *æ¯ä¸€ä¸ª* <cN> æ®µè½è¾“å‡ºå¯¹åº” <cN> æ®µè½ï¼Œä¿æŒé¡ºåºä¸€è‡´ã€‚  
            â€¢ ç»ä¸å¯åˆå¹¶ã€å¢åˆ æˆ–è·³è¿‡æ®µè½ã€‚è‹¥ç¡®å®æ— æ³•ç¿»è¯‘ï¼ŒåŸæ–‡ç”¨ <cN>{{{{MISSING}}}}</cN> åŸæ ·æŠ„å†™ã€‚

            2. **ä¸çœç•¥**  
            â€¢ è¯‘æ–‡è¡Œæ•° â‰ˆ æºè¡Œæ•°ã€‚  
            â€¢ ç»“å°¾è‡ªè¡Œæ‰§è¡Œæ£€æŸ¥ï¼šè‹¥å‘ç°æœ‰æœªè¾“å‡ºçš„ <cX> æ®µï¼Œå¿…é¡»è¡¥ä¸Š <cX>{{{{MISSING}}}}</cX>ã€‚

            3. **æ™ºèƒ½è¯†åˆ«ä¸å¤„ç†**ï¼ˆé‡è¦ï¼ï¼‰
            â€¢ **é¡µçœ‰é¡µè„šæ ‡è®°**ï¼šé‡åˆ°ä»¥ä¸‹å†…å®¹è¾“å‡ºç‰¹æ®Šæ ‡è®° <cN>[é¡µçœ‰é¡µè„š]</cN>ï¼š
              - é¡µç ä¿¡æ¯ï¼ˆå¦‚"Page 1 of 506"ã€"ç¬¬1é¡µ/å…±506é¡µ"ç­‰ï¼‰
              - ä½œè€…ä¿¡æ¯é‡å¤ï¼ˆå¦‚é‚®ç®±åœ°å€ã€ä½œè€…åé‡å¤å‡ºç°ï¼‰
              - ç½‘ç«™é“¾æ¥ã€ç‰ˆæƒä¿¡æ¯
              - æ˜æ˜¾çš„é¡µçœ‰é¡µè„šé‡å¤å†…å®¹
            â€¢ **ç« èŠ‚æ ‡é¢˜è¯†åˆ«**ï¼šè¯†åˆ«ä»¥ä¸‹å†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼š
              - ç« èŠ‚æ ‡é¢˜ â†’ ## æ ‡é¢˜
              - å°èŠ‚æ ‡é¢˜ â†’ ### æ ‡é¢˜  
              - "Chapter X"ã€"ç¬¬Xç« " â†’ ## ç¬¬Xç« 
              - å±…ä¸­çš„çŸ­æ ‡é¢˜ â†’ ### æ ‡é¢˜
            â€¢ **ç‰¹æ®Šå†…å®¹å¤„ç†**ï¼š
              - ä½œè€…çš„è¯ã€å‰è¨€ã€åè®°ç­‰ â†’ ## ä½œè€…çš„è¯
              - ç›®å½•ã€ç´¢å¼•ç­‰ â†’ [ç›®å½•]

            4. **æœ¯è¯­è¡¨**ï¼ˆglossaryï¼‰  
            â€¢ è§ä¸‹æ–¹ã€Šæœ¯è¯­è¡¨ã€‹ï¼›è‹¥è¯æ¡å·²åˆ—å‡ºï¼Œåˆ™åœ¨è¯‘æ–‡åŸæ ·ä¿ç•™ï¼Œä¸å¾—è¯‘ã€‚  
            â€¢ å¦‚é‡æ–°ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å“ç‰Œåç­‰ï¼‰ï¼š**åœ¨è¯‘æ–‡ä¸­ä¿æŒåŸè¯ä¸ç¿»è¯‘ï¼Œå¹¶åœ¨ ```glossary``` ä¸­æŒ‰æ ¼å¼ åŸè¯â‡¢åŸè¯ æ ‡è®°**ï¼Œè„šæœ¬åç»­ä¼šå¢é‡å†™å…¥æœ¯è¯­è¡¨ã€‚
            â€¢ æ³¨æ„ï¼šä¸“æœ‰åè¯åº”ä¿æŒåŸæ–‡ï¼Œä¸è¦ç¿»è¯‘æˆä¸­æ–‡ã€‚

            5. **é£æ ¼å®ˆåˆ™**  
            â€¢ ä¿æŒåŸæ–‡é£æ ¼ç‰¹å¾ï¼š{style_cache}
            â€¢ **ç¬¬ä¸‰äººç§°æ”¹ç¬¬ä¸€äººç§°**ï¼šæ³°è¯­è½¬è¯‘ä¸­å¸¸è§ç”¨ç¬¬ä¸‰äººç§°ç§°å‘¼è‡ªå·±çš„å¯¹è¯ï¼Œå¿…é¡»æ”¹æˆç¬¬ä¸€äººç§°ä»¥ç¬¦åˆä¸­æ–‡é˜…è¯»ä¹ æƒ¯ï¼ˆæ­¤è§„åˆ™ä¼˜å…ˆçº§é«˜äºæœ¯è¯­è¡¨ä¿ç•™åŸè¯ï¼‰ã€‚
            â€¢ **æ ‡ç‚¹è§„èŒƒ**ï¼šç”¨ä¸­æ–‡æ ‡ç‚¹ï¼Œè‹±æ–‡ä¸“åå†…éƒ¨ä¿ç•™åŠè§’ã€‚ç¦æ­¢å‡ºç°å¤šä¸ªè¿ç»­å¥å·ï¼ˆå¦‚ã€‚ã€‚ã€‚ã€.ã€‚ã€‚ã€‚ã€.ã€‚.ç­‰ï¼‰ï¼Œç»Ÿä¸€ä½¿ç”¨çœç•¥å·â€¦â€¦ã€‚  
            â€¢ æ•°å­—ã€è®¡é‡å•ä½ã€è´§å¸ç¬¦å·ç…§åŸæ–‡ã€‚
            â€¢ ä¿æŒåŸæ–‡çš„å™äº‹èŠ‚å¥ã€è¯­è°ƒå’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹å¼ã€‚

            =============== æœ¯è¯­è¡¨ï¼ˆä¾›å‚è€ƒï¼‰ ===============
            {gloss_block}

            ===== è¾“å‡ºæ ¼å¼ç¤ºä¾‹ =====
            è¾“å…¥ï¼š
            <c1>Page 1 of 506</c1>
            <c2>Author Name</c2>
            <c3>Chapter 1: The Beginning</c3>
            <c4>It was a dark and stormy night...</c4>
            
            è¾“å‡ºï¼š
            <c1>[é¡µçœ‰é¡µè„š]</c1>
            <c2>[é¡µçœ‰é¡µè„š]</c2>
            <c3># ç¬¬ä¸€ç« ï¼šå¼€å§‹</c3>
            <c4>é‚£æ˜¯ä¸€ä¸ªé»‘æš—è€Œæš´é£é›¨çš„å¤œæ™šâ€¦â€¦</c4>
            
            ===== ä¸¥æ ¼éµå®ˆè¾“å‡ºæ ¼å¼ =====
            <c1>ç¬¬ä¸€æ®µè¯‘æ–‡æˆ–ç©º</c1>
            <c2>ç¬¬äºŒæ®µè¯‘æ–‡æˆ–ç©º</c2>
            ...
            ```glossary
            ä¸“æœ‰åè¯1â‡¢ä¸“æœ‰åè¯1
            ä¸“æœ‰åè¯2â‡¢ä¸“æœ‰åè¯2
            ```
            
            **ä¸“æœ‰åè¯å¤„ç†ç¤ºä¾‹**ï¼š
            - äººå "John Smith" â†’ è¯‘æ–‡ä¸­ä¿æŒ "John Smith"ï¼Œæœ¯è¯­è¡¨ä¸­æ·»åŠ  "John Smithâ‡¢John Smith"
            - åœ°å "Bangkok" â†’ è¯‘æ–‡ä¸­ä¿æŒ "Bangkok"ï¼Œæœ¯è¯­è¡¨ä¸­æ·»åŠ  "Bangkokâ‡¢Bangkok"
            - å“ç‰Œ "iPhone" â†’ è¯‘æ–‡ä¸­ä¿æŒ "iPhone"ï¼Œæœ¯è¯­è¡¨ä¸­æ·»åŠ  "iPhoneâ‡¢iPhone"
            """.strip())

        # --- è°ƒç”¨ LLM ---
        try:
            logging.info(f"ğŸ¤– å¼€å§‹ç¿»è¯‘æ‰¹æ¬¡{batch_num}ï¼Œå†…å®¹é•¿åº¦: {len(tagged_eng)} å­—ç¬¦")
            llm_out = call_llm(system_prompt, tagged_eng)
            
            if not llm_out or not llm_out.strip():
                raise ValueError("LLMè¿”å›å†…å®¹ä¸ºç©º")
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡{batch_num}ç¿»è¯‘å¤±è´¥: {e}")
            # åˆ›å»ºé”™è¯¯å ä½ç¬¦
            MISSING_DICT[batch_id] = ["ç¿»è¯‘å¤±è´¥"]
            error_content = f"**ç¿»è¯‘å¤±è´¥**: {e}\n\nåŸæ–‡:\n{raw_eng[:500]}...\n"
            
            batch_path = CHAP_DIR / f"{batch_id}.md"
            batch_path.write_text(error_content, encoding="utf-8")
            big_md_parts.append(error_content)
            
            logging.warning(f"æ‰¹æ¬¡{batch_num}å·²ä¿å­˜é”™è¯¯å ä½ç¬¦")
            continue

        # --- æ¸…æ´— & è§£æ ---
        try:
            cn_body, new_terms_block, miss = strip_tags(llm_out, keep_missing=True)
            MISSING_DICT[batch_id] = miss
            
            # éªŒè¯ç¿»è¯‘è´¨é‡
            if not cn_body.strip():
                raise ValueError("ç¿»è¯‘ç»“æœä¸ºç©º")
            
            # æ£€æŸ¥æ®µè½æ•°é‡æ˜¯å¦åˆç†
            original_segments = len(re.findall(r'<c\d+>', tagged_eng))
            translated_segments = len(re.findall(r'<c\d+>', llm_out))
            
            if abs(original_segments - translated_segments) > original_segments * 0.2:  # å…è®¸20%çš„å·®å¼‚
                warning_msg = f"åŸæ–‡{original_segments}æ®µ vs è¯‘æ–‡{translated_segments}æ®µ"
                logging.warning(f"æ‰¹æ¬¡{batch_num}æ®µè½æ•°é‡å·®å¼‚è¾ƒå¤§: {warning_msg}")
                WARNING_DICT[batch_id] = warning_msg
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡{batch_num}ç»“æœè§£æå¤±è´¥: {e}")
            MISSING_DICT[batch_id] = ["è§£æå¤±è´¥"]
            cn_body = f"**è§£æå¤±è´¥**: {e}\n\nåŸå§‹LLMè¾“å‡º:\n{llm_out[:1000]}..."
            new_terms_block = ""

        # --- æ›´æ–°æœ¯è¯­è¡¨ ---
        try:
            new_terms_count = 0
            for line in new_terms_block.splitlines():
                if "\t" in line or "â‡¢" in line:
                    # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šåˆ¶è¡¨ç¬¦åˆ†éš”æˆ–ç®­å¤´åˆ†éš”
                    if "â‡¢" in line:
                        src, tgt = [x.strip() for x in line.split("â‡¢", 1)]
                    else:
                        src, tgt = [x.strip() for x in line.split("\t", 1)]
                    
                    if src and tgt and src not in GLOSSARY:
                        GLOSSARY[src] = tgt
                        new_terms_count += 1
            
            if new_terms_count > 0:
                logging.info(f"ğŸ“š æ‰¹æ¬¡{batch_num}æ–°å¢{new_terms_count}ä¸ªæœ¯è¯­")
                
        except Exception as e:
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æœ¯è¯­è¡¨æ›´æ–°å¤±è´¥: {e}")

        # --- å†™æ‰¹æ¬¡æ–‡ä»¶ ---
        try:
            batch_path = CHAP_DIR / f"{batch_id}.md"
            batch_content = f"{cn_body}\n"
            batch_path.write_text(batch_content, encoding="utf-8")
            big_md_parts.append(batch_content)
            
            # éªŒè¯æ–‡ä»¶å†™å…¥
            if not batch_path.exists() or batch_path.stat().st_size == 0:
                raise IOError("æ–‡ä»¶å†™å…¥å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º")
            
            logging.info(f"âœ… æ‰¹æ¬¡ {batch_num} ç¿»è¯‘å®Œæˆ â†’ {batch_path.name}")
            if miss:
                logging.warning(f"âš ï¸  æ‰¹æ¬¡ {batch_num} æœ‰ {len(miss)} ä¸ªç¼ºå¤±æ®µè½")
            
        except Exception as e:
            logging.error(f"âŒ æ‰¹æ¬¡{batch_num}æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
            raise
        
        # ä¿å­˜è¿›åº¦ï¼ˆæ¯å¤„ç†å®Œä¸€ç« å°±ä¿å­˜æœ¯è¯­è¡¨ï¼‰
        try:
            save_glossary(GLOSSARY, gloss_path)
        except Exception as e:
            logging.warning(f"æœ¯è¯­è¡¨ä¿å­˜å¤±è´¥: {e}")
        
        # é€‚åº¦é™é€Ÿ
        time.sleep(1)
        
    except Exception as e:
        logging.error(f"å¤„ç†æ‰¹æ¬¡{batch_num}æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
        MISSING_DICT[batch_id] = [f"å¤„ç†é”™è¯¯: {str(e)}"]
        continue

# ========= æ±‡æ€»è¾“å‡º ========= #
logging.info("=== å¼€å§‹ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š ===")

# ç»Ÿè®¡ä¿¡æ¯
processed_batches_success = len([bid for bid in MISSING_DICT if not any("å¤„ç†é”™è¯¯" in str(m) for m in MISSING_DICT[bid])])  # æ’é™¤ä¸¥é‡é”™è¯¯çš„æ‰¹æ¬¡
failed_batches = [bid for bid, miss_list in MISSING_DICT.items() if any("å¤±è´¥" in str(m) or "é”™è¯¯" in str(m) for m in miss_list)]
missing_segments = sum(len([m for m in miss_list if m != "ç¿»è¯‘å¤±è´¥" and "é”™è¯¯" not in str(m)]) for miss_list in MISSING_DICT.values())

# 1. æœ¯è¯­è¡¨
try:
    save_glossary(GLOSSARY, gloss_path)
    logging.info(f"æœ¯è¯­è¡¨å·²æ›´æ–° â†’ {gloss_path} (å…±{len(GLOSSARY)}ä¸ªæ¡ç›®)")
except Exception as e:
    logging.error(f"æœ¯è¯­è¡¨ä¿å­˜å¤±è´¥: {e}")

# 2. ç¼ºå¤±æ®µè½æ¸…å•
try:
    missing_report = [
        "# ç¿»è¯‘è´¨é‡æŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"æ€»æ‰¹æ¬¡æ•°: {total_batches}",
        f"æˆåŠŸå¤„ç†: {processed_batches_success}",
        f"å¤±è´¥æ‰¹æ¬¡: {len(failed_batches)}",
        f"ç¼ºå¤±æ®µè½: {missing_segments}",
        f"æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH}",
        "",
        "## è¯¦ç»†ä¿¡æ¯"
    ]
    
    # å¤±è´¥æ‰¹æ¬¡
    if failed_batches:
        missing_report.extend(["", "### å¤±è´¥æ‰¹æ¬¡"])
        for bid in sorted(failed_batches):
            missing_report.append(f"- {bid}: {', '.join(MISSING_DICT[bid])}")
    
    # ç¼ºå¤±æ®µè½
    batches_with_missing = {bid: miss_list for bid, miss_list in MISSING_DICT.items() 
                           if miss_list and not any("å¤±è´¥" in str(m) or "é”™è¯¯" in str(m) for m in miss_list)}
    
    if batches_with_missing:
        missing_report.extend(["", "### ç¼ºå¤±æ®µè½"])
        for bid in sorted(batches_with_missing):
            if batches_with_missing[bid]:
                missing_report.append(f"- {bid}: {', '.join(batches_with_missing[bid])}")
    
    # æ®µè½æ•°é‡å·®å¼‚è­¦å‘Š
    if WARNING_DICT:
        missing_report.extend(["", "### æ®µè½æ•°é‡å·®å¼‚è­¦å‘Š"])
        for bid in sorted(WARNING_DICT):
            missing_report.append(f"- {bid}: {WARNING_DICT[bid]}")
    
    # æˆåŠŸæ‰¹æ¬¡
    successful_batches = [bid for bid in MISSING_DICT if bid not in failed_batches and not MISSING_DICT[bid]]
    if successful_batches:
        missing_report.extend(["", "### å®Œå…¨æˆåŠŸæ‰¹æ¬¡"])
        missing_report.append(f"å…±{len(successful_batches)}æ‰¹æ¬¡: {', '.join(sorted(successful_batches))}")
    
    missing_path = OUT_DIR / "translation_report.txt"
    missing_path.write_text("\n".join(missing_report), encoding="utf-8")
    logging.info(f"ç¿»è¯‘æŠ¥å‘Šå·²ä¿å­˜ â†’ {missing_path}")
except Exception as e:
    logging.error(f"ç¿»è¯‘æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

# 3. åˆå¹¶Markdown
try:
    if big_md_parts:
        # æ·»åŠ æ–‡æ¡£å¤´éƒ¨
        header = f"""# ç¿»è¯‘æ–‡æ¡£

> ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> æ€»æ‰¹æ¬¡: {total_batches} | æˆåŠŸ: {processed_batches_success} | å¤±è´¥: {len(failed_batches)}
> æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH} | æ€»é¡µæ•°: {total_pages}

---

"""
        
        big_md_content = header + "\n".join(big_md_parts)
        big_md_path = OUT_DIR / BIG_MD_NAME
        big_md_path.write_text(big_md_content, encoding="utf-8")
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        file_size = big_md_path.stat().st_size
        logging.info(f"å…¨é›† Markdown æ±‡æ€»å®Œæˆ â†’ {big_md_path} ({file_size:,} å­—èŠ‚)")
    else:
        logging.warning("æ²¡æœ‰æˆåŠŸç¿»è¯‘çš„æ‰¹æ¬¡ï¼Œè·³è¿‡Markdownæ±‡æ€»")
except Exception as e:
    logging.error(f"Markdownæ±‡æ€»å¤±è´¥: {e}")

# 4. æœ€ç»ˆç»Ÿè®¡
logging.info("=== ç¿»è¯‘æµç¨‹å®Œæˆ ===")
log_progress(total_batches, total_batches, "æœ€ç»ˆè¿›åº¦", "å®Œæˆ")
logging.info(f"âœ… å¤„ç†ç»“æœ: {processed_batches_success}/{total_batches} æ‰¹æ¬¡æˆåŠŸ")
if failed_batches:
    logging.warning(f"âŒ å¤±è´¥æ‰¹æ¬¡: {', '.join(failed_batches)}")
if missing_segments > 0:
    logging.warning(f"âš ï¸  æ€»è®¡ç¼ºå¤±æ®µè½: {missing_segments}")
else:
    logging.info("ğŸ‰ æ‰€æœ‰æ®µè½ç¿»è¯‘å®Œæˆï¼")

# 5. ç”Ÿæˆé‡è¯•è„šæœ¬ï¼ˆå¦‚æœæœ‰å¤±è´¥æ‰¹æ¬¡ï¼‰
if failed_batches:
    try:
        retry_config = CONFIG.copy()
        # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡ç”Ÿæˆé‡è¯•é…ç½®
        retry_batches = []
        for bid in failed_batches:
            if bid.startswith("batch_"):
                batch_num = int(bid.split("_")[1])
                p_start = (batch_num - 1) * PAGES_PER_BATCH + 1
                p_end = min(batch_num * PAGES_PER_BATCH, total_pages)
                retry_batches.append({"batch_num": batch_num, "pages": [p_start, p_end]})
        
        retry_config["failed_batches"] = retry_batches
        retry_config["pages_per_batch"] = PAGES_PER_BATCH
        
        retry_config_path = OUT_DIR / "retry_config.json"
        with open(retry_config_path, 'w', encoding='utf-8') as f:
            json.dump(retry_config, f, ensure_ascii=False, indent=2)
        
        logging.info(f"é‡è¯•é…ç½®å·²ç”Ÿæˆ â†’ {retry_config_path}")
        logging.info("å¯ä½¿ç”¨æ­¤é…ç½®é‡æ–°è¿è¡Œè„šæœ¬å¤„ç†å¤±è´¥æ‰¹æ¬¡")
    except Exception as e:
        logging.warning(f"é‡è¯•é…ç½®ç”Ÿæˆå¤±è´¥: {e}")

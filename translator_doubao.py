#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
translator_doubao.py
â€”â€” ä½¿ç”¨è±†åŒ…(doubao-seed-1-6)æ¨¡å‹çš„PDFç¿»è¯‘è„šæœ¬
  â€¢ æŒ‰æŒ‡å®šé¡µæ•°Xè‡ªåŠ¨åˆ†æ‰¹ç¿»è¯‘
  â€¢ å¤„ç†å¥å­å®Œæ•´æ€§ï¼Œç¡®ä¿ç¿»è¯‘è‡³å¥å­ç»“æŸ
  â€¢ è‡ªåŠ¨è¯†åˆ«æ ‡é¢˜å¹¶ä¿æŒæ ¼å¼
  â€¢ è‡ªåŠ¨ç¼–å·å¹¶æœ€ç»ˆæ•´åˆä¸ºä¸€ä¸ªæ–‡ä»¶
  â€¢ ä½¿ç”¨promptçº¦æŸä¸“æœ‰åè¯å¤„ç†
  â€¢ é€‚é…è±†åŒ…APIæ ¼å¼
"""
import json, re, textwrap, logging, time, datetime, requests, os, sys, warnings, random
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# è¿‡æ»¤PyMuPDFçš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning, module="fitz")

import fitz  # PyMuPDF - pip install PyMuPDF
from pdf_crop_tool import PDFCropTool  # å¯¼å…¥PDFè£åˆ‡å·¥å…·

# æˆæœ¬è·Ÿè¸ªå…¨å±€å˜é‡
total_cost = 0.0
total_input_tokens = 0
total_output_tokens = 0

# ========= ç¼“å­˜ç®¡ç†åŠŸèƒ½ ========= #
def clean_cache_files(cache_dir: Path, pdf_path: Path = None, force: bool = False):
    """æ¸…ç†ç¼“å­˜æ–‡ä»¶
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•
        pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
    """
    if not cache_dir.exists():
        return
    
    cache_patterns = [
        "*_text_cache.txt",      # PDFæ–‡æœ¬ç¼“å­˜
        "batch_*_raw_text.txt",  # æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜
    ]
    
    cleaned_count = 0
    total_size_cleaned = 0
    
    logging.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ç¼“å­˜æ–‡ä»¶ (å¼ºåˆ¶æ¸…ç†: {'æ˜¯' if force else 'å¦'})")
    
    for pattern in cache_patterns:
        for cache_file in cache_dir.glob(pattern):
            should_clean = force
            
            if not should_clean and pdf_path and pdf_path.exists():
                try:
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    pdf_mtime = pdf_path.stat().st_mtime
                    cache_mtime = cache_file.stat().st_mtime
                    should_clean = cache_mtime < pdf_mtime
                except Exception:
                    should_clean = True  # å‡ºé”™æ—¶æ¸…ç†
            
            if should_clean:
                try:
                    file_size = cache_file.stat().st_size
                    cache_file.unlink()
                    cleaned_count += 1
                    total_size_cleaned += file_size
                    logging.info(f"ğŸ—‘ï¸  åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file.name} ({file_size/1024:.1f}KB)")
                except Exception as e:
                    logging.warning(f"âš ï¸  æ¸…ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
    
    if cleaned_count > 0:
        logging.info(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_size_cleaned/1024:.1f}KB ç©ºé—´")
    else:
        logging.info("ğŸ’¾ æ— éœ€æ¸…ç†ç¼“å­˜æ–‡ä»¶")

# ========= è¯»å–é…ç½® ========= #
def load_config(config_file: str = None) -> Dict:
    """å®‰å…¨åŠ è½½é…ç½®æ–‡ä»¶"""
    ROOT = Path(__file__).resolve().parent
    
    if config_file:
        config_path = Path(config_file)
        if not config_path.is_absolute():
            config_path = ROOT / config_file
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œå°è¯•ä»å‘½ä»¤è¡Œå‚æ•°è·å–
        if len(sys.argv) > 1:
            config_path = Path(sys.argv[1])
            if not config_path.is_absolute():
                config_path = ROOT / sys.argv[1]
        else:
            # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„config.json
            config_path = ROOT / "config.json"
    
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    try:
        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)
        
        # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        required_keys = ["api", "paths"]
        for key in required_keys:
            if key not in config:
                raise KeyError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
        
        # éªŒè¯APIé…ç½®
        api_config = config["api"]
        required_api_keys = ["API_URL", "API_KEY", "LLM_MODEL"]
        for key in required_api_keys:
            if key not in api_config:
                raise KeyError(f"APIé…ç½®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
        
        # éªŒè¯è·¯å¾„é…ç½®
        paths_config = config["paths"]
        required_path_keys = ["pdf", "output_dir"]
        for key in required_path_keys:
            if key not in paths_config:
                raise KeyError(f"è·¯å¾„é…ç½®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
        
        logging.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
        
    except json.JSONDecodeError as e:
        raise ValueError(f"é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# ========= é…ç½®åŠ è½½å’ŒéªŒè¯ ========= #
try:
    CONFIG = load_config()
except Exception as e:
    print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
    print("\nè¯·é€‰æ‹©é…ç½®æ–‡ä»¶:")
    print("1. ä½¿ç”¨é»˜è®¤é…ç½® (config.json)")
    print("2. è¾“å…¥è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        try:
            CONFIG = load_config("config.json")
        except Exception as e:
            print(f"âŒ é»˜è®¤é…ç½®åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    elif choice == "2":
        config_path = input("è¯·è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„: ").strip()
        try:
            CONFIG = load_config(config_path)
        except Exception as e:
            print(f"âŒ è‡ªå®šä¹‰é…ç½®åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        sys.exit(1)

# æå–é…ç½®
API_URL = CONFIG["api"]["API_URL"]
API_KEY = CONFIG["api"]["API_KEY"]
LLM_MODEL = CONFIG["api"]["LLM_MODEL"]
TEMPERATURE = CONFIG["api"].get("temperature", 0.2)

PDF_PATH = Path(CONFIG["paths"]["pdf"])
OUT_DIR = Path(CONFIG["paths"]["output_dir"])
BIG_MD_NAME = CONFIG["paths"].get("big_md_name", "translated_document.md")

PAGES_PER_BATCH = CONFIG.get("pages_per_batch", 8)

# PDFè£åˆ‡é…ç½®
PDF_CROP_CONFIG = CONFIG.get("pdf_crop", {})
ENABLE_PDF_CROP = PDF_CROP_CONFIG.get("enable", False)
CROP_MARGINS = PDF_CROP_CONFIG.get("margins", {"top": 50, "bottom": 50, "left": 30, "right": 30})
AUTO_DETECT_HEADERS = PDF_CROP_CONFIG.get("auto_detect_headers", True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUT_DIR / "translation.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# éªŒè¯æ–‡ä»¶è·¯å¾„
if not PDF_PATH.exists():
    raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {PDF_PATH}")

logging.info(f"=== è±†åŒ…PDFç¿»è¯‘å™¨å¯åŠ¨ ===")
logging.info(f"PDFæ–‡ä»¶: {PDF_PATH}")
logging.info(f"è¾“å‡ºç›®å½•: {OUT_DIR}")
logging.info(f"æ¨¡å‹: {LLM_MODEL}")
logging.info(f"æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH}")
logging.info(f"PDFè£åˆ‡: {'å¯ç”¨' if ENABLE_PDF_CROP else 'ç¦ç”¨'}")

# åˆ›å»ºè¾“å‡ºç›®å½•
try:
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    CHAP_DIR = OUT_DIR / "chap_md"
    CHAP_DIR.mkdir(exist_ok=True)
    RAW_CONTENT_DIR = OUT_DIR / "raw_content"
    RAW_CONTENT_DIR.mkdir(exist_ok=True)
    logging.info(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {OUT_DIR}")
    logging.info(f"åŸå§‹å†…å®¹ç›®å½•å·²å‡†å¤‡: {RAW_CONTENT_DIR}")
except Exception as e:
    raise RuntimeError(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")

# æ¸…ç†ç¼“å­˜
if CONFIG.get("clean_cache_on_start", True):
    logging.info("=== æ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶ ===")
    clean_cache_files(RAW_CONTENT_DIR, PDF_PATH, force=False)

# æœ¯è¯­è¡¨å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
logging.info("æœ¯è¯­è¡¨åŠŸèƒ½å·²ç¦ç”¨ï¼Œä½¿ç”¨promptçº¦æŸä¸“æœ‰åè¯å¤„ç†")

# ========= PDFæ–‡æœ¬æå– ========= #
def get_pdf_text_with_cache(pdf_path: Path, cache_dir: Path) -> str:
    """ä»PDFæå–æ–‡æœ¬ï¼Œæ”¯æŒç¼“å­˜å’Œè£åˆ‡åŠŸèƒ½"""
    cache_file = cache_dir / f"{pdf_path.stem}_text_cache.txt"
    
    # æ£€æŸ¥ç¼“å­˜
    if cache_file.exists():
        try:
            pdf_mtime = pdf_path.stat().st_mtime
            cache_mtime = cache_file.stat().st_mtime
            
            if cache_mtime >= pdf_mtime:
                logging.info(f"ğŸ’¾ ä½¿ç”¨PDFæ–‡æœ¬ç¼“å­˜: {cache_file.name}")
                return cache_file.read_text(encoding="utf-8")
            else:
                logging.info(f"ğŸ”„ PDFæ–‡ä»¶å·²æ›´æ–°ï¼Œé‡æ–°æå–æ–‡æœ¬")
        except Exception as e:
            logging.warning(f"æ£€æŸ¥ç¼“å­˜æ—¶é—´æˆ³å¤±è´¥: {e}ï¼Œé‡æ–°æå–")
    
    logging.info(f"ğŸ“– å¼€å§‹æå–PDFæ–‡æœ¬: {pdf_path.name}")
    
    try:
        doc = fitz.open(pdf_path)
        all_text = []
        
        # åˆå§‹åŒ–PDFè£åˆ‡å·¥å…·
        crop_tool = None
        if ENABLE_PDF_CROP:
            try:
                crop_tool = PDFCropTool(str(pdf_path))
                if AUTO_DETECT_HEADERS:
                    logging.info("ğŸ” è‡ªåŠ¨æ£€æµ‹é¡µçœ‰é¡µè„šå¹¶è£åˆ‡")
                    crop_tool.auto_crop_headers_footers()
                else:
                    logging.info(f"âœ‚ï¸  æ‰‹åŠ¨è£åˆ‡è¾¹è·: {CROP_MARGINS}")
                    crop_tool.crop_pages(**CROP_MARGINS)
            except Exception as e:
                logging.warning(f"PDFè£åˆ‡åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡µé¢: {e}")
                crop_tool = None
        
        for page_num in range(len(doc)):
            try:
                page = doc[page_num]
                
                # å¦‚æœå¯ç”¨äº†è£åˆ‡ï¼Œä½¿ç”¨è£åˆ‡åçš„é¡µé¢
                if crop_tool:
                    try:
                        # è·å–è£åˆ‡åçš„æ–‡æœ¬
                        blocks = page.get_text("blocks", sort=True)
                        page_text = "\n".join([block[4] for block in blocks if block[4].strip()])
                    except Exception as e:
                        logging.warning(f"é¡µé¢{page_num+1}è£åˆ‡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {e}")
                        page_text = page.get_text("blocks", sort=True)
                        page_text = "\n".join([block[4] for block in page_text if block[4].strip()])
                else:
                    # ä½¿ç”¨åŸå§‹é¡µé¢
                    blocks = page.get_text("blocks", sort=True)
                    page_text = "\n".join([block[4] for block in blocks if block[4].strip()])
                
                all_text.append(page_text)
                
            except Exception as e:
                logging.error(f"æå–ç¬¬{page_num+1}é¡µæ–‡æœ¬å¤±è´¥: {e}")
                all_text.append("")
        
        # å…³é—­è£åˆ‡å·¥å…·
        if crop_tool:
            try:
                crop_tool.close()
            except Exception as e:
                logging.warning(f"å…³é—­PDFè£åˆ‡å·¥å…·å¤±è´¥: {e}")
        
        doc.close()
        
        # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬ï¼Œä½¿ç”¨\fåˆ†éš”
        full_text = "\f".join(all_text)
        
        # ä¿å­˜ç¼“å­˜
        try:
            cache_file.write_text(full_text, encoding="utf-8")
            logging.info(f"ğŸ’¾ PDFæ–‡æœ¬å·²ç¼“å­˜: {cache_file.name}")
        except Exception as e:
            logging.warning(f"ä¿å­˜PDFæ–‡æœ¬ç¼“å­˜å¤±è´¥: {e}")
        
        logging.info(f"âœ… PDFæ–‡æœ¬æå–å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        return full_text
        
    except Exception as e:
        raise RuntimeError(f"PDFæ–‡æœ¬æå–å¤±è´¥: {e}")

# ========= è±†åŒ…APIè°ƒç”¨ ========= #
def call_llm(prompt_sys: str, prompt_user: str, max_retries: int = 3, timeout: int = 120) -> str:
    """è°ƒç”¨è±†åŒ…LLM APIï¼Œå¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†"""
    global total_cost, total_input_tokens, total_output_tokens
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    # è±†åŒ…APIæ ¼å¼ - æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¼˜åŒ–
    payload = {
        "model": "doubao-seed-1-6-250615",
        "messages": [
            {"role": "system", "content": prompt_sys},
            {"role": "user", "content": prompt_user}
        ],
        "temperature": TEMPERATURE,
        "thinking": False,  # å…³é—­æ·±åº¦æ€è€ƒæ¨¡å¼ä»¥æé«˜ç¿»è¯‘é€Ÿåº¦
        "max_completion_tokens": 8000,  # é™åˆ¶è¾“å‡ºé•¿åº¦
        "stream": False
    }
    
    last_error = None
    for attempt in range(max_retries):
        try:
            logging.info(f"ğŸ¤– è°ƒç”¨è±†åŒ…API - æ¨¡å‹: doubao-seed-1-6 (å°è¯• {attempt+1}/{max_retries})")
            logging.debug(f"ç³»ç»Ÿæç¤ºé•¿åº¦: {len(prompt_sys)} å­—ç¬¦")
            logging.debug(f"ç”¨æˆ·è¾“å…¥é•¿åº¦: {len(prompt_user)} å­—ç¬¦")
            
            resp = requests.post(API_URL, headers=headers, json=payload, timeout=timeout)
            resp.raise_for_status()
            
            result = resp.json()
            if "choices" not in result or not result["choices"]:
                raise ValueError("APIè¿”å›æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ")
            
            content = result["choices"][0]["message"]["content"]
            if not content or not content.strip():
                raise ValueError("APIè¿”å›å†…å®¹ä¸ºç©º")
            
            logging.info(f"âœ… è±†åŒ…å“åº”æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è®°å½•tokenä½¿ç”¨æƒ…å†µå’Œè®¡ç®—æˆæœ¬
            if "usage" in result and result["usage"]:
                usage = result["usage"]
                input_tokens = usage.get('prompt_tokens', 0)
                output_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', 0)
                
                # ç´¯è®¡tokenç»Ÿè®¡
                total_input_tokens += input_tokens
                total_output_tokens += output_tokens
                
                # è®¡ç®—æˆæœ¬ï¼ˆå¦‚æœå¯ç”¨äº†æˆæœ¬è·Ÿè¸ªï¼‰
                if CONFIG.get('pricing', {}).get('enable_cost_tracking', False):
                    pricing = CONFIG.get('pricing', {})
                    input_price_per_1k = pricing.get('input_price_per_1k_tokens', 0)
                    output_price_per_1k = pricing.get('output_price_per_1k_tokens', 0)
                    currency = pricing.get('currency', 'USD')
                    
                    batch_input_cost = (input_tokens / 1000) * input_price_per_1k
                    batch_output_cost = (output_tokens / 1000) * output_price_per_1k
                    batch_total_cost = batch_input_cost + batch_output_cost
                    
                    total_cost += batch_total_cost
                    
                    logging.info(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥{input_tokens} + è¾“å‡º{output_tokens} = æ€»è®¡{total_tokens}")
                    logging.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: {batch_total_cost:.4f} {currency} (è¾“å…¥: {batch_input_cost:.4f} + è¾“å‡º: {batch_output_cost:.4f})")
                    logging.info(f"ğŸ’³ ç´¯è®¡æˆæœ¬: {total_cost:.4f} {currency}")
                else:
                    logging.info(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥{input_tokens} + è¾“å‡º{output_tokens} = æ€»è®¡{total_tokens}")
            
            return content.strip()
            
        except requests.exceptions.Timeout:
            last_error = f"è¯·æ±‚è¶…æ—¶({timeout}ç§’)"
            logging.warning(f"è±†åŒ…APIè°ƒç”¨è¶…æ—¶({attempt+1}/{max_retries})")
        except requests.exceptions.ConnectionError:
            last_error = "ç½‘ç»œè¿æ¥é”™è¯¯"
            logging.warning(f"è±†åŒ…APIè°ƒç”¨ç½‘ç»œé”™è¯¯({attempt+1}/{max_retries})")
        except requests.exceptions.HTTPError as e:
            last_error = f"HTTPé”™è¯¯: {e.response.status_code}"
            logging.warning(f"è±†åŒ…APIè°ƒç”¨HTTPé”™è¯¯({attempt+1}/{max_retries}): {e}")
        except (KeyError, ValueError, json.JSONDecodeError) as e:
            last_error = f"å“åº”è§£æé”™è¯¯: {e}"
            logging.warning(f"è±†åŒ…APIè°ƒç”¨è§£æé”™è¯¯({attempt+1}/{max_retries}): {e}")
        except Exception as e:
            last_error = f"æœªçŸ¥é”™è¯¯: {e}"
            logging.warning(f"è±†åŒ…APIè°ƒç”¨æœªçŸ¥é”™è¯¯({attempt+1}/{max_retries}): {e}")
        
        if attempt < max_retries - 1:
            wait_time = (attempt + 1) * 2
            logging.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    raise RuntimeError(f"è±†åŒ…APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {last_error}")

# ========= å…¶ä»–è¾…åŠ©å‡½æ•° ========= #
def log_progress(current: int, total: int, task_name: str, current_info: str = "", start_time: float = None):
    """è®°å½•è¿›åº¦ä¿¡æ¯"""
    percentage = (current / total) * 100 if total > 0 else 0
    progress_bar = "â–ˆ" * int(percentage // 5) + "â–‘" * (20 - int(percentage // 5))
    
    time_info = ""
    if start_time:
        elapsed = time.time() - start_time
        if current > 0:
            avg_time = elapsed / current
            remaining = (total - current) * avg_time
            time_info = f" | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ"
    
    logging.info(f"ğŸ“Š {task_name}: [{progress_bar}] {percentage:.1f}% ({current}/{total}){time_info} {current_info}")

def ensure_sentence_completion_optimized(text: str, next_batch_preview: str = "", max_supplement: int = 50) -> str:
    """ä¼˜åŒ–çš„å¥å­å®Œæ•´æ€§å¤„ç†ï¼Œé™åˆ¶è¡¥å……å­—ç¬¦æ•°é‡"""
    if not text or not text.strip():
        return text
    
    # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾
    sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n']
    
    # å¦‚æœå·²ç»ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾ï¼Œç›´æ¥è¿”å›
    if any(text.rstrip().endswith(ending) for ending in sentence_endings):
        return text
    
    # å¦‚æœæ²¡æœ‰ä¸‹ä¸€æ‰¹æ¬¡é¢„è§ˆæ–‡æœ¬ï¼Œæ·»åŠ å¥å·
    if not next_batch_preview:
        return text + "."
    
    # é™åˆ¶æœç´¢èŒƒå›´ï¼Œé¿å…è¿‡åº¦è¯»å–
    search_text = next_batch_preview[:max_supplement]
    
    # æŸ¥æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸä½ç½®
    best_pos = -1
    best_ending = ""
    
    for ending in sentence_endings:
        pos = search_text.find(ending)
        if pos != -1 and (best_pos == -1 or pos < best_pos):
            best_pos = pos
            best_ending = ending
    
    # å¦‚æœæ‰¾åˆ°å¥å­ç»“æŸç¬¦ï¼Œè¡¥å……åˆ°è¯¥ä½ç½®
    if best_pos != -1:
        supplement = search_text[:best_pos + len(best_ending)]
        logging.info(f"ğŸ“ å¥å­å®Œæ•´æ€§å¤„ç†: è¡¥å…… {len(supplement)} å­—ç¬¦")
        return text + supplement
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å¥å­ç»“æŸç¬¦ï¼Œæ·»åŠ å¥å·
    logging.info("ğŸ“ å¥å­å®Œæ•´æ€§å¤„ç†: æœªæ‰¾åˆ°å¥å­ç»“æŸç¬¦ï¼Œæ·»åŠ å¥å·")
    return text + "."

# ä½¿ç”¨ä¼˜åŒ–åçš„å‡½æ•°
ensure_sentence_completion = ensure_sentence_completion_optimized

def wrap_batch_with_tags(text: str) -> str:
    """ä¸ºæ‰¹æ¬¡æ–‡æœ¬æ·»åŠ æ®µè½æ ‡ç­¾"""
    if not text or not text.strip():
        return text
    
    # æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œç¬¦æˆ–å•æ¢è¡Œç¬¦ï¼‰
    paragraphs = re.split(r'\n\s*\n|\n', text)
    
    # è¿‡æ»¤ç©ºæ®µè½
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    
    if not paragraphs:
        return text
    
    # æ·»åŠ æ ‡ç­¾
    tagged_paragraphs = []
    for i, paragraph in enumerate(paragraphs, 1):
        tagged_paragraphs.append(f"<c{i}>{paragraph}</c{i}>")
    
    return "\n\n".join(tagged_paragraphs)

def refresh_style(sample_text: str) -> str:
    """åˆ†ææ–‡æœ¬é£æ ¼"""
    global style_cache
    
    if not sample_text or len(sample_text) < 100:
        style_cache = ""
        return style_cache
    
    # ç®€å•çš„é£æ ¼åˆ†æ
    style_indicators = []
    
    # æ£€æŸ¥å¯¹è¯æ¯”ä¾‹
    dialogue_count = len(re.findall(r'["""\'\'\'].*?["""\'\'\']', sample_text))
    total_sentences = len(re.findall(r'[.!?ã€‚ï¼ï¼Ÿ]', sample_text))
    
    if total_sentences > 0:
        dialogue_ratio = dialogue_count / total_sentences
        if dialogue_ratio > 0.3:
            style_indicators.append("å¯¹è¯è¾ƒå¤š")
        elif dialogue_ratio > 0.1:
            style_indicators.append("é€‚é‡å¯¹è¯")
        else:
            style_indicators.append("å™è¿°ä¸ºä¸»")
    
    # æ£€æŸ¥å¥å­é•¿åº¦
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', sample_text)
    avg_length = sum(len(s.strip()) for s in sentences if s.strip()) / max(len([s for s in sentences if s.strip()]), 1)
    
    if avg_length > 50:
        style_indicators.append("é•¿å¥è¾ƒå¤š")
    elif avg_length > 25:
        style_indicators.append("å¥å­é€‚ä¸­")
    else:
        style_indicators.append("çŸ­å¥ä¸ºä¸»")
    
    style_cache = f"ï¼Œæ–‡æœ¬ç‰¹å¾ï¼š{', '.join(style_indicators)}"
    logging.info(f"ğŸ“ æ–‡æœ¬é£æ ¼åˆ†æ: {style_cache}")
    return style_cache

# åˆå§‹åŒ–é£æ ¼ç¼“å­˜
style_cache = ""

# ========= ä¸»è¦å¤„ç†é€»è¾‘ ========= #

# åŠ è½½PDF
logging.info(f"å¼€å§‹åŠ è½½PDFæ–‡ä»¶: {PDF_PATH}")
try:
    # æå–PDFæ–‡æœ¬
    full_text = get_pdf_text_with_cache(PDF_PATH, RAW_CONTENT_DIR)
    if not full_text or not full_text.strip():
        raise ValueError("PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
    
    pages = re.split(r"\f", full_text)                           # PyMuPDF æŒ‰ formfeed åˆ†é¡µ
    if pages and pages[-1] == "":
        pages.pop()
    
    logging.info(f"PDFåŠ è½½å®Œæˆï¼Œå…±{len(pages)}é¡µ")
    
    # éªŒè¯åˆ†æ‰¹è®¾ç½®
    max_page = len(pages)
    if PAGES_PER_BATCH < 1:
        raise ValueError(f"æ¯æ‰¹é¡µæ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰è®¾ç½®: {PAGES_PER_BATCH}")
    
    logging.info(f"å°†æŒ‰æ¯{PAGES_PER_BATCH}é¡µè¿›è¡Œåˆ†æ‰¹ç¿»è¯‘ï¼ŒPDFå…±{max_page}é¡µ")
    
except Exception as e:
    raise RuntimeError(f"PDFå¤„ç†å¤±è´¥: {e}")

# åˆå§‹åŒ–å¤„ç†å˜é‡
MISSING_DICT = {}
WARNING_DICT = {}  # æ”¶é›†æ®µè½æ•°é‡å·®å¼‚ç­‰è­¦å‘Šä¿¡æ¯
big_md_parts = []
total_pages = len(pages)
total_batches = (total_pages + PAGES_PER_BATCH - 1) // PAGES_PER_BATCH  # å‘ä¸Šå–æ•´
processed_batches = 0

logging.info(f"=== å¼€å§‹åˆ†æ‰¹ç¿»è¯‘å¤„ç† ===")
logging.info(f"æ€»æ‰¹æ¬¡: {total_batches} | æ¯æ‰¹é¡µæ•°: {PAGES_PER_BATCH} | æ€»é¡µæ•°: {total_pages}")

# å¼€å§‹è®¡æ—¶
batch_start_time = time.time()

def get_batch_text_with_cache(pages: List[str], batch_num: int, p_start: int, p_end: int, cache_dir: Path) -> str:
    """è·å–æ‰¹æ¬¡æ–‡æœ¬ï¼Œæ”¯æŒç¼“å­˜"""
    batch_id = f"batch_{batch_num:03d}"
    cache_file = cache_dir / f"{batch_id}_raw_text.txt"
    
    # æ£€æŸ¥ç¼“å­˜
    if cache_file.exists():
        try:
            cached_text = cache_file.read_text(encoding="utf-8")
            if cached_text.strip():
                logging.info(f"ğŸ’¾ ä½¿ç”¨æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜: {batch_id}")
                return cached_text
        except Exception as e:
            logging.warning(f"è¯»å–æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°ç”Ÿæˆ")
    
    # ç”Ÿæˆæ‰¹æ¬¡æ–‡æœ¬
    batch_pages = pages[(p_start-1):p_end]  # è½¬æ¢ä¸º0ç´¢å¼•
    batch_text = "\n\n".join(batch_pages)
    
    # ä¿å­˜ç¼“å­˜
    try:
        cache_file.write_text(batch_text, encoding="utf-8")
        logging.info(f"ğŸ’¾ æ‰¹æ¬¡æ–‡æœ¬å·²ç¼“å­˜: {batch_id}")
    except Exception as e:
        logging.warning(f"ä¿å­˜æ‰¹æ¬¡æ–‡æœ¬ç¼“å­˜å¤±è´¥: {e}")
    
    return batch_text

# åˆ†æ‰¹å¤„ç†
for batch_num in range(1, total_batches + 1):
    processed_batches += 1
    
    # è®¡ç®—é¡µé¢èŒƒå›´
    p_start = (batch_num - 1) * PAGES_PER_BATCH + 1
    p_end = min(batch_num * PAGES_PER_BATCH, total_pages)
    
    # è®°å½•è¿›åº¦
    logging.info(f"=== å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (é¡µ {p_start}-{p_end}) ===")
    log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", f"å½“å‰: æ‰¹æ¬¡{batch_num}", batch_start_time)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¿»è¯‘ç»“æœ
    batch_id = f"batch_{batch_num:03d}"
    batch_md_file = CHAP_DIR / f"{batch_id}.md"
    if batch_md_file.exists():
        try:
            cached_content = batch_md_file.read_text(encoding="utf-8")
            if cached_content.strip():
                logging.info(f"ğŸ’¾ æ‰¹æ¬¡ {batch_num} å·²å­˜åœ¨ç¿»è¯‘ç»“æœï¼Œè·³è¿‡å¤„ç†")
                big_md_parts.append(cached_content)
                log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "ä½¿ç”¨ç¼“å­˜", batch_start_time)
                continue
        except Exception as e:
            logging.warning(f"è¯»å–æ‰¹æ¬¡ç¿»è¯‘ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°å¤„ç†")
    
    try:
        # è·å–æ‰¹æ¬¡åŸå§‹è‹±æ–‡æ–‡æœ¬
        raw_eng = get_batch_text_with_cache(pages, batch_num, p_start, p_end, RAW_CONTENT_DIR)
        
        if not raw_eng.strip():
            logging.warning(f"ğŸ“„ æ‰¹æ¬¡{batch_num}å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
            MISSING_DICT[batch_id] = ["æ•´æ‰¹ç¼ºå¤±"]
            log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "å†…å®¹ä¸ºç©º", batch_start_time)
            continue
        
        # å¥å­å®Œæ•´æ€§å¤„ç†
        if batch_num < total_batches:  # ä¸æ˜¯æœ€åä¸€ä¸ªæ‰¹æ¬¡
            try:
                next_p_start = batch_num * PAGES_PER_BATCH + 1
                next_p_end = min((batch_num + 1) * PAGES_PER_BATCH, total_pages)
                
                next_batch_text = get_batch_text_with_cache(pages, batch_num + 1, next_p_start, next_p_end, RAW_CONTENT_DIR)
                # åªä¼ é€’ä¸‹ä¸€æ‰¹æ¬¡çš„å‰1000å­—ç¬¦ç”¨äºå¥å­å®Œæ•´æ€§æ£€æŸ¥
                next_batch_preview = next_batch_text[:1000] if next_batch_text else ""
                
                raw_eng = ensure_sentence_completion(raw_eng, next_batch_preview)
            except Exception as e:
                logging.warning(f"è·å–ä¸‹ä¸€æ‰¹æ¬¡æ–‡æœ¬å¤±è´¥ï¼Œè·³è¿‡å¥å­å®Œæ•´æ€§å¤„ç†: {e}")
                # å¦‚æœè·å–ä¸‹ä¸€æ‰¹æ¬¡å¤±è´¥ï¼Œä»ç„¶è¿›è¡ŒåŸºæœ¬çš„å¥å­å®Œæ•´æ€§å¤„ç†
                raw_eng = ensure_sentence_completion(raw_eng)
        
        # æ·»åŠ æ®µè½æ ‡ç­¾
        tagged_eng = wrap_batch_with_tags(raw_eng)
        
        # æ£€æŸ¥åˆ†æ®µç»“æœ
        tag_count = len(re.findall(r'<c\d+>', tagged_eng))
        if tag_count == 0:
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æœªèƒ½æ­£ç¡®åˆ†æ®µ")
        else:
            logging.info(f"ğŸ“ æ‰¹æ¬¡{batch_num}åˆ†ä¸º{tag_count}ä¸ªæ®µè½ï¼Œæ–‡æœ¬é•¿åº¦: {len(raw_eng)} å­—ç¬¦")
        
        # é£æ ¼åˆ†æï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        if not style_cache:
            logging.info("ğŸ“ å¼€å§‹æ–‡æœ¬é£æ ¼åˆ†æ...")
            text_length = len(raw_eng)
            sample_length = min(5000, text_length)  # æ ·æœ¬é•¿åº¦ä¸è¶…è¿‡æ–‡æœ¬æ€»é•¿åº¦
            
            if text_length > sample_length:
                # ä»æ–‡æœ¬ä¸­é—´éšæœºå–æ ·ï¼Œé¿å…å¼€å¤´å¯èƒ½çš„ç›®å½•æˆ–ç»“å°¾çš„ç‰ˆæƒä¿¡æ¯
                start_range_begin = int(text_length * 0.2)
                start_range_end = int(text_length * 0.8) - sample_length
                
                if start_range_end > start_range_begin:
                    start_pos = random.randint(start_range_begin, start_range_end)
                    sample_text = raw_eng[start_pos:start_pos + sample_length]
                    logging.info(f"ğŸ“ ä»æ–‡æœ¬ä¸­é—´éšæœºå–æ ·è¿›è¡Œé£æ ¼åˆ†æ (ä½ç½®: {start_pos}-{start_pos + sample_length})")
                else:
                    # å¦‚æœèŒƒå›´å¤ªå°ï¼Œä»ä¸­é—´å–æ ·
                    start_pos = max(0, (text_length - sample_length) // 2)
                    sample_text = raw_eng[start_pos:start_pos + sample_length]
                    logging.info(f"ğŸ“ ä»æ–‡æœ¬ä¸­é—´å–æ ·è¿›è¡Œé£æ ¼åˆ†æ (ä½ç½®: {start_pos}-{start_pos + sample_length})")
            else:
                # æ–‡æœ¬è¾ƒçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹
                sample_text = raw_eng
                logging.info("ğŸ“ æ–‡æœ¬è¾ƒçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹è¿›è¡Œé£æ ¼åˆ†æ")
            
            refresh_style(sample_text)
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = textwrap.dedent( f"""
            ä½ æ˜¯èµ„æ·±æ–‡å­¦è¯‘è€…ï¼Œå°†ä¸‹æ–¹å°è¯´ç²¾å‡†ä¼˜é›…åœ°è¯‘æˆç°ä»£ä¸­æ–‡ã€‚
            
            ===== æ ¸å¿ƒè¦æ±‚ =====
            1. **æ®µè½å¯¹é½**ï¼šæ¯ä¸ª<cN>æ®µè½å¿…é¡»å¯¹åº”è¾“å‡º<cN>æ®µè½ï¼Œä¸å¯åˆå¹¶/è·³è¿‡ã€‚æ— æ³•ç¿»è¯‘æ—¶ç”¨<cN>{{{{MISSING}}}}</cN>ã€‚
            2. **å®Œæ•´æ€§**ï¼šå¿…é¡»ç¿»è¯‘æ‰€æœ‰æ®µè½ï¼Œç‰¹åˆ«æ³¨æ„æœ€åä¸€æ®µï¼å®Œæˆåæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼ã€‚
            3. **ä¸“æœ‰åè¯å¤„ç†**ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
               â€¢ äººåï¼šä¿æŒè‹±æ–‡åŸæ–‡ä¸ç¿»è¯‘ï¼ˆå¦‚ï¼šJohn, Mary, Smithç­‰ï¼‰
               â€¢ æ³°è¯­ç§°å‘¼ï¼šä¿æŒåŸæ–‡ï¼ˆå¦‚ï¼šKhun, P', N', Phi, Nong, Ajarn, Krub, Kaç­‰ï¼‰
            4. **é¡µçœ‰é¡µè„šå¤„ç†**ï¼š
               â€¢ é¡µç  â†’ <cN>[é¡µçœ‰é¡µè„š]</cN>
               â€¢ é‡å¤çš„ä½œè€…ç½²å â†’ <cN>[é¡µçœ‰é¡µè„š]</cN>
               â€¢ å…¶ä»–ä¸æ­£æ–‡æ— å…³çš„å…ƒæ•°æ® â†’ <cN>[é¡µçœ‰é¡µè„š]</cN>
            5. **ç‰¹æ®Šæ ‡è®°**ï¼š
               â€¢ ç« èŠ‚æ ‡é¢˜ â†’ ## ç¬¬XXç«  æ ‡é¢˜ï¼ˆä¸¥æ ¼ä¸¤ä½æ•°ç¼–å·ï¼š01ã€02ã€03...ï¼Œå»é™¤æ‰€æœ‰è£…é¥°ç¬¦å·ï¼‰
               â€¢ åºç« /å°¾å£°/ä½œè€…è¯ â†’ ## åºç«  / ## å°¾å£° / ## ä½œè€…çš„è¯
               â€¢ ç•ªå¤–/ç‰¹åˆ«ç¯‡/å¤–ä¼  â†’ ## ç•ªå¤–01 æ ‡é¢˜å†…å®¹ / ## ç‰¹åˆ«ç¯‡01 æ ‡é¢˜å†…å®¹ / ## å¤–ä¼ 01 æ ‡é¢˜å†…å®¹
               â€¢ ä½œè€…çš„è¯ã€å‰è¨€ã€åè®°ç­‰ â†’ ## ä½œè€…çš„è¯
               â€¢ ç›®å½•ã€ç´¢å¼•ç­‰ â†’ [ç›®å½•]
               â€¢ åˆ†éš”ç¬¦ â†’ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ï¼ˆç»Ÿä¸€ä½¿ç”¨6ä¸ªé•¿æ¨ªçº¿ï¼‰
            6. **æ–‡å­¦æ€§**ï¼šè¿½æ±‚éŸµå¾‹ç¾æ„Ÿï¼Œå‡†ç¡®ä¼ è¾¾æƒ…æ„Ÿï¼Œä¿ç•™ä¿®è¾æ‰‹æ³•ï¼Œè¥é€ æ„å¢ƒï¼Œé€‚åº”ä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ã€‚
            7. **é£æ ¼**ï¼šä¿æŒåŸæ–‡ç‰¹å¾{style_cache}ï¼Œç¬¬ä¸‰äººç§°å¯¹è¯æ”¹ç¬¬ä¸€äººç§°ï¼Œä¸­æ–‡æ ‡ç‚¹ï¼Œè¿ç»­å¥å·æ”¹çœç•¥å·ï¼Œä¼˜é€‰æ–‡å­¦è¯æ±‡ã€‚
            
            ===== é‡è¦æé†’ =====
            â€¢ å¿…é¡»å°†æ‰€æœ‰æ®µè½ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¾“å‡º<cN>æ ‡ç­¾æ•°é‡ä¸è¾“å…¥å®Œå…¨å¯¹åº”
            â€¢ æ— æ³•ç¿»è¯‘æ—¶ä½¿ç”¨<cN>{{{{MISSING}}}}</cN>
            â€¢ å¦‚æœé‡åˆ°æ— æ³•ç†è§£çš„å†…å®¹ï¼Œä½¿ç”¨<cN>{{{{MISSING}}}}</cN>è€Œä¸æ˜¯è·³è¿‡
            â€¢ ç»å¯¹ä¸èƒ½è·³è¿‡ä»»ä½•æ®µè½ï¼Œå³ä½¿å†…å®¹å¾ˆçŸ­æˆ–çœ‹ä¼¼ä¸é‡è¦
            â€¢ ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å“ç‰Œåã€æœºæ„åã€æ³°è¯­ç§°å‘¼ï¼‰å¿…é¡»ä¿æŒåŸæ–‡ä¸ç¿»è¯‘
            
            ===== è¾“å‡ºæ ¼å¼ =====
            <c1>ç¬¬ä¸€æ®µè¯‘æ–‡</c1>
            <c2>ç¬¬äºŒæ®µè¯‘æ–‡</c2>
            ...
            <cN>æœ€åä¸€æ®µè¯‘æ–‡</cN>
            """).strip()
        
        # è°ƒç”¨è±†åŒ…APIè¿›è¡Œç¿»è¯‘
        logging.info(f"ğŸ¤– å¼€å§‹ç¿»è¯‘æ‰¹æ¬¡ {batch_num}...")
        translated_content = call_llm(system_prompt, tagged_eng)
        
        if not translated_content or not translated_content.strip():
            logging.error(f"âŒ æ‰¹æ¬¡{batch_num}ç¿»è¯‘ç»“æœä¸ºç©º")
            MISSING_DICT[batch_id] = ["ç¿»è¯‘ç»“æœä¸ºç©º"]
            continue
        
        # éªŒè¯ç¿»è¯‘ç»“æœ
        input_tags = len(re.findall(r'<c\d+>', tagged_eng))
        output_tags = len(re.findall(r'<c\d+>', translated_content))
        
        if input_tags != output_tags:
            warning_msg = f"æ®µè½æ•°é‡ä¸åŒ¹é…: è¾“å…¥{input_tags}ä¸ªï¼Œè¾“å‡º{output_tags}ä¸ª"
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num} {warning_msg}")
            WARNING_DICT[batch_id] = warning_msg
        
        # ä¿å­˜ç¿»è¯‘ç»“æœ
        try:
            batch_md_file.write_text(translated_content, encoding="utf-8")
            logging.info(f"ğŸ’¾ æ‰¹æ¬¡{batch_num}ç¿»è¯‘ç»“æœå·²ä¿å­˜: {batch_md_file.name}")
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜æ‰¹æ¬¡{batch_num}ç¿»è¯‘ç»“æœå¤±è´¥: {e}")
            continue
        
        # æ·»åŠ åˆ°æœ€ç»ˆç»“æœ
        big_md_parts.append(translated_content)
        
        logging.info(f"âœ… æ‰¹æ¬¡{batch_num}å¤„ç†å®Œæˆ")
        log_progress(processed_batches, total_batches, "æ‰¹æ¬¡è¿›åº¦", "ç¿»è¯‘å®Œæˆ", batch_start_time)
        
    except Exception as e:
        logging.error(f"âŒ æ‰¹æ¬¡{batch_num}å¤„ç†å¤±è´¥: {e}")
        MISSING_DICT[batch_id] = [f"å¤„ç†å¤±è´¥: {e}"]
        continue

# ========= æœ€ç»ˆæ•´åˆå’Œç»Ÿè®¡ ========= #

logging.info("=== å¼€å§‹æœ€ç»ˆæ•´åˆ ===")

# åˆå¹¶æ‰€æœ‰ç¿»è¯‘ç»“æœ
if big_md_parts:
    final_content = "\n\n".join(big_md_parts)
    
    # æ·»åŠ æ–‡æ¡£å¤´éƒ¨ä¿¡æ¯
    header = textwrap.dedent(f"""
        # ç¿»è¯‘æ–‡æ¡£
        
        **ç¿»è¯‘æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        **æºæ–‡ä»¶**: {PDF_PATH.name}
        **æ¨¡å‹**: doubao-seed-1-6
        **æ€»é¡µæ•°**: {total_pages}
        **æ€»æ‰¹æ¬¡**: {total_batches}
        
        ---
        
        """)
    
    final_content = header + final_content
    
    # ä¿å­˜æœ€ç»ˆæ–‡æ¡£
    final_md_path = OUT_DIR / BIG_MD_NAME
    try:
        final_md_path.write_text(final_content, encoding="utf-8")
        logging.info(f"ğŸ“„ æœ€ç»ˆç¿»è¯‘æ–‡æ¡£å·²ä¿å­˜: {final_md_path}")
    except Exception as e:
        logging.error(f"âŒ ä¿å­˜æœ€ç»ˆæ–‡æ¡£å¤±è´¥: {e}")
else:
    logging.error("âŒ æ²¡æœ‰ä»»ä½•ç¿»è¯‘å†…å®¹å¯ä»¥æ•´åˆ")

# ç»Ÿè®¡ä¿¡æ¯
logging.info("=== ç¿»è¯‘ç»Ÿè®¡ ===")
logging.info(f"ğŸ“Š æ€»é¡µæ•°: {total_pages}")
logging.info(f"ğŸ“Š æ€»æ‰¹æ¬¡: {total_batches}")
logging.info(f"ğŸ“Š æˆåŠŸæ‰¹æ¬¡: {len(big_md_parts)}")
logging.info(f"ğŸ“Š å¤±è´¥æ‰¹æ¬¡: {len(MISSING_DICT)}")
logging.info(f"ğŸ“Š è­¦å‘Šæ‰¹æ¬¡: {len(WARNING_DICT)}")

# Tokenå’Œæˆæœ¬ç»Ÿè®¡
if total_input_tokens > 0 or total_output_tokens > 0:
    logging.info(f"ğŸ“Š æ€»è¾“å…¥Token: {total_input_tokens:,}")
    logging.info(f"ğŸ“Š æ€»è¾“å‡ºToken: {total_output_tokens:,}")
    logging.info(f"ğŸ“Š æ€»Token: {total_input_tokens + total_output_tokens:,}")
    
    if CONFIG.get('pricing', {}).get('enable_cost_tracking', False):
        currency = CONFIG.get('pricing', {}).get('currency', 'USD')
        logging.info(f"ğŸ’° æ€»æˆæœ¬: {total_cost:.4f} {currency}")

# å¤±è´¥å’Œè­¦å‘Šè¯¦æƒ…
if MISSING_DICT:
    logging.warning("âš ï¸  å¤±è´¥æ‰¹æ¬¡è¯¦æƒ…:")
    for batch_id, errors in MISSING_DICT.items():
        logging.warning(f"   {batch_id}: {', '.join(errors)}")

if WARNING_DICT:
    logging.warning("âš ï¸  è­¦å‘Šæ‰¹æ¬¡è¯¦æƒ…:")
    for batch_id, warning in WARNING_DICT.items():
        logging.warning(f"   {batch_id}: {warning}")

# ç”Ÿæˆé‡è¯•è„šæœ¬
if MISSING_DICT:
    retry_script_path = OUT_DIR / "retry_failed_batches.py"
    retry_script_content = textwrap.dedent(f"""
        #!/usr/bin/env python3
        # -*- coding: utf-8 -*-
        '''
        é‡è¯•å¤±è´¥æ‰¹æ¬¡çš„ç¿»è¯‘è„šæœ¬
        è‡ªåŠ¨ç”Ÿæˆäº: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        '''
        
        import os
        import sys
        from pathlib import Path
        
        # å¤±è´¥çš„æ‰¹æ¬¡åˆ—è¡¨
        failed_batches = {list(MISSING_DICT.keys())}
        
        print(f"æ£€æµ‹åˆ° {{len(failed_batches)}} ä¸ªå¤±è´¥æ‰¹æ¬¡:")
        for batch in failed_batches:
            print(f"  - {{batch}}")
        
        print("\nè¦é‡è¯•è¿™äº›æ‰¹æ¬¡ï¼Œè¯·:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
        print("2. åˆ é™¤å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰")
        print("3. é‡æ–°è¿è¡Œä¸»ç¿»è¯‘è„šæœ¬")
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªåŠ¨é‡è¯•é€»è¾‘
        """)
    
    try:
        retry_script_path.write_text(retry_script_content, encoding="utf-8")
        logging.info(f"ğŸ“ é‡è¯•è„šæœ¬å·²ç”Ÿæˆ: {retry_script_path}")
    except Exception as e:
        logging.warning(f"âš ï¸  ç”Ÿæˆé‡è¯•è„šæœ¬å¤±è´¥: {e}")

# è®¡ç®—æ€»è€—æ—¶
total_time = time.time() - batch_start_time
logging.info(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")

logging.info("=== è±†åŒ…PDFç¿»è¯‘å®Œæˆ ===")

if len(big_md_parts) == total_batches:
    logging.info("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡ç¿»è¯‘æˆåŠŸï¼")
else:
    logging.warning(f"âš ï¸  éƒ¨åˆ†æ‰¹æ¬¡ç¿»è¯‘å¤±è´¥ï¼ŒæˆåŠŸç‡: {len(big_md_parts)/total_batches*100:.1f}%")

logging.info(f"ğŸ“„ ç¿»è¯‘ç»“æœä¿å­˜åœ¨: {OUT_DIR}")
logging.info(f"ğŸ“„ æœ€ç»ˆæ–‡æ¡£: {OUT_DIR / BIG_MD_NAME}")
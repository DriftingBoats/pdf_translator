#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿé‡æ–°ç¿»è¯‘æŒ‡å®šæ‰¹æ¬¡

ç”¨æ³•:
  python retranslate_batch.py 9           # é‡æ–°ç¿»è¯‘æ‰¹æ¬¡9
  python retranslate_batch.py 9 12 15     # é‡æ–°ç¿»è¯‘æ‰¹æ¬¡9ã€12ã€15
  python retranslate_batch.py --all-diff  # é‡æ–°ç¿»è¯‘æ‰€æœ‰å·®å¼‚è¾ƒå¤§çš„æ‰¹æ¬¡
"""

import json
import re
import textwrap
import logging
import time
import requests
import sys
import argparse
import glob
from pathlib import Path
from typing import Dict, List

# æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
TAG_PAT = re.compile(r'<c\d+>(.*?)</c\d+>', re.DOTALL)
NEWTERM_PAT = re.compile(r'```glossary\s*\n(.*?)\n```', re.DOTALL | re.IGNORECASE)

# è®¾ç½®ç®€æ´çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(message)s',
    datefmt='%H:%M:%S'
)

def load_config(config_path: str = "config.json") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error(f"âŒ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logging.error(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        sys.exit(1)

def call_llm(prompt_sys: str, prompt_user: str, config: dict) -> str:
    """è°ƒç”¨LLM API"""
    headers = {
        "Authorization": f"Bearer {config['api']['API_KEY']}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": config['api']['LLM_MODEL'],
        "messages": [
            {"role": "system", "content": prompt_sys},
            {"role": "user", "content": prompt_user}
        ],
        "temperature": config['api'].get('temperature', 0.3),
        "max_tokens": 4000
    }
    
    try:
        response = requests.post(
            config['api']['API_URL'],
            headers=headers,
            json=data,
            timeout=120
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content']
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            
    except Exception as e:
        raise Exception(f"APIè°ƒç”¨å¼‚å¸¸: {e}")

def wrap_batch_with_tags(raw_text: str) -> str:
    """æŠŠæ‰¹æ¬¡åŸæ–‡æŒ‰ç©ºè¡Œåˆ†æ®µï¼ŒåŠ  <c1>â€¦</c1> æ ‡ç­¾ï¼Œè®©LLMè‡ªè¡Œè¯†åˆ«æ ‡é¢˜å’Œé¡µçœ‰é¡µç """
    segments = [seg.strip() for seg in re.split(r"\n\s*\n", raw_text) if seg.strip()]
    tagged = []
    
    for idx, seg in enumerate(segments, start=1):
        # ä¸å†é¢„å…ˆæ ‡è®°æ ‡é¢˜ï¼Œè®©LLMè‡ªè¡Œè¯†åˆ«å’Œå¤„ç†
        tagged.append(f"<c{idx}>{seg}</c{idx}>")
    
    return "\n\n".join(tagged)

def count_segments(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬ä¸­çš„æ®µè½æ•°é‡"""
    # å¦‚æœæ–‡æœ¬åŒ…å«æ ‡ç­¾ï¼ŒæŒ‰æ ‡ç­¾è®¡ç®—
    if '<c' in text and '>' in text:
        return len(re.findall(r'<c\d+>', text))
    # å¦åˆ™æŒ‰éç©ºè¡Œè®¡ç®—æ®µè½æ•°
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return len(lines)

def strip_tags(llm_output: str, keep_missing: bool = True):
    """æ¸…æ´— LLM è¾“å‡º & æ”¶é›†ç¼ºå¤±æ®µ"""
    paragraphs = TAG_PAT.findall(llm_output)

    miss_list, clean_paras = [], []
    for idx, p in enumerate(paragraphs, start=1):
        if p.strip() == "{{MISSING}}":
            miss_list.append(f"c{idx:03d}")
            if keep_missing:
                clean_paras.append("{{MISSING}}")
        elif p.strip() == "" or p.strip().startswith("[é¡µçœ‰é¡µè„š]") or p.strip().startswith("[ç›®å½•]"):  # å¤„ç†ç©ºæ ‡ç­¾å’Œç‰¹æ®Šæ ‡è®°
            # è·³è¿‡ç©ºå†…å®¹å’Œé¡µçœ‰é¡µè„šã€ç›®å½•æ ‡è®°ï¼Œä¸æ·»åŠ åˆ°clean_parasä¸­
            pass
        else:
            clean_paras.append(p.strip())

    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…å¤šä½™çš„ç©ºè¡Œ
    clean_paras = [para for para in clean_paras if para.strip()]
    pure_text = "\n\n".join(clean_paras)
    new_terms_block = "\n".join(
        line.strip()
        for blk in NEWTERM_PAT.findall(llm_output)
        for line in blk.strip().splitlines() if line.strip()
    )
    return pure_text, new_terms_block, miss_list

def find_diff_batches(output_dir: Path, threshold: float = 0.2) -> List[int]:
    """æŸ¥æ‰¾æ®µè½æ•°é‡å·®å¼‚è¾ƒå¤§çš„æ‰¹æ¬¡"""
    chap_dir = output_dir / "chap_md"
    raw_content_dir = output_dir / "raw_content"
    
    problem_batches = []
    
    for batch_file in sorted(chap_dir.glob("batch_*.md")):
        batch_num = int(re.search(r'batch_(\d+)', batch_file.name).group(1))
        
        try:
            # è¯»å–ç¿»è¯‘ç»“æœ
            translated_content = batch_file.read_text(encoding='utf-8')
            translated_segments = count_segments(translated_content)
            
            # è¯»å–åŸå§‹æ–‡æœ¬
            raw_file = raw_content_dir / f"batch_{batch_num:03d}.txt"
            if raw_file.exists():
                raw_content = raw_file.read_text(encoding='utf-8')
                original_segments = count_segments(raw_content)
                
                # è®¡ç®—å·®å¼‚æ¯”ä¾‹
                if original_segments > 0:
                    diff_ratio = abs(original_segments - translated_segments) / original_segments
                    if diff_ratio > threshold:
                        problem_batches.append(batch_num)
                        
        except Exception:
            continue
    
    return sorted(problem_batches)

def load_glossary(glossary_path: Path) -> Dict[str, str]:
    """åŠ è½½æœ¯è¯­è¡¨"""
    glossary = {}
    if glossary_path.exists():
        try:
            content = glossary_path.read_text(encoding='utf-8')
            for line in content.strip().split('\n'):
                if '\t' in line:
                    key, value = line.split('\t', 1)
                    glossary[key.strip()] = value.strip()
        except Exception:
            pass
    return glossary

def merge_markdown(config: dict, output_dir: Path) -> bool:
    """åˆå¹¶ç¿»è¯‘åçš„markdownæ–‡ä»¶ä¸ºæœ€ç»ˆæ–‡æ¡£"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰chap_mdç›®å½•ï¼ˆå¤šä¹¦ç±åœºæ™¯ï¼‰
        chap_md_dir = output_dir / "chap_md"
        if chap_md_dir.exists():
            # å¤šä¹¦ç±åœºæ™¯ï¼šä»chap_mdç›®å½•åˆå¹¶
            chap_files = sorted(glob.glob(str(chap_md_dir / "*.md")))
            big_md_path = output_dir / config['paths']['big_md_name']
        else:
            # å•ä¹¦ç±åœºæ™¯ï¼šä»è¾“å‡ºç›®å½•ç›´æ¥åˆå¹¶
            chap_files = sorted(glob.glob(str(output_dir / "*.md")))
            big_md_path = output_dir / config['paths']['big_md_name']
        
        if not chap_files:
            logging.warning("âš ï¸  æœªæ‰¾åˆ°éœ€è¦åˆå¹¶çš„markdownæ–‡ä»¶")
            return False
        
        logging.info(f"ğŸ“ å¼€å§‹åˆå¹¶ {len(chap_files)} ä¸ªmarkdownæ–‡ä»¶...")
        
        with open(big_md_path, "w", encoding="utf-8") as wf:
            # æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
            wf.write("å…¨æ–‡æœºç¿»  \næ›´å¤šæ³°ç™¾å°è¯´è§ `https://thaigl.drifting.boats/`\n\n---\n\n")
            
            for fp in chap_files:
                content = Path(fp).read_text(encoding="utf-8").strip()
                if content:
                    wf.write(content + "\n\n")
        
        logging.info(f"âœ… å·²ç”Ÿæˆæ•´ä¹¦ Markdownï¼š{big_md_path}")
        return True
        
    except Exception as e:
        logging.error(f"âŒ åˆå¹¶markdownæ–‡ä»¶å¤±è´¥: {e}")
        return False

def retranslate_batch(batch_num: int, config: dict, output_dir: str, glossary: Dict[str, str]) -> bool:
    """é‡æ–°ç¿»è¯‘æŒ‡å®šæ‰¹æ¬¡"""
    output_path = Path(output_dir)
    raw_content_dir = output_path / "raw_content"
    chap_dir = output_path / "chap_md"
    
    # è¯»å–åŸå§‹æ–‡æœ¬
    raw_file = raw_content_dir / f"batch_{batch_num:03d}.txt"
    if not raw_file.exists():
        logging.error(f"âŒ æ‰¹æ¬¡ {batch_num} åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        raw_content = raw_file.read_text(encoding='utf-8')
        # ä½¿ç”¨ä¸translator.pyç›¸åŒçš„æ–¹å¼è®¡ç®—æ®µè½æ•°
        tagged_content = wrap_batch_with_tags(raw_content)
        original_segments = count_segments(tagged_content)
        logging.info(f"ğŸ“– å¼€å§‹é‡æ–°ç¿»è¯‘æ‰¹æ¬¡ {batch_num} (åŸæ–‡ {original_segments} æ®µ)")
    except Exception as e:
        logging.error(f"âŒ è¯»å–æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")
        return False
    
    # æ„å»ºæœ¯è¯­è¡¨
    gloss_block = "\n".join(f"{k}\t{v}" for k, v in glossary.items())
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    system_prompt = textwrap.dedent(f"""
        ä½ æ˜¯ä¸€åèµ„æ·±æ–‡å­¦è¯‘è€…ï¼Œéœ€è¦å°†è‹±æ–‡å°è¯´ç²¾å‡†ç¿»è¯‘æˆä¸­æ–‡ã€‚

        **æ ¸å¿ƒè¦æ±‚**ï¼š
        1. é€æ®µè½å¯¹é½ï¼šæ¯ä¸ª <cN> æ ‡ç­¾å¿…é¡»æœ‰å¯¹åº”çš„ç¿»è¯‘è¾“å‡º
        2. ä¸èƒ½åˆå¹¶ã€åˆ é™¤æˆ–è·³è¿‡ä»»ä½•æ®µè½
        3. å¦‚æœæ— æ³•ç¿»è¯‘ï¼Œç”¨ <cN>{{{{MISSING}}}}</cN> æ ‡è®°
        4. é¡µçœ‰é¡µè„šç”¨ <cN>[é¡µçœ‰é¡µè„š]</cN> æ ‡è®°
        5. ç« èŠ‚æ ‡é¢˜è½¬æ¢ä¸º Markdown æ ¼å¼
        6. ä¸“æœ‰åè¯ä¿æŒåŸæ–‡ï¼Œåœ¨æœ¯è¯­è¡¨ä¸­æ ‡è®°

        **æœ¯è¯­è¡¨**ï¼š
        {gloss_block}

        **è¾“å‡ºæ ¼å¼**ï¼š
        <c1>ç¬¬ä¸€æ®µè¯‘æ–‡</c1>
        <c2>ç¬¬äºŒæ®µè¯‘æ–‡</c2>
        ...
        ```glossary
        æ–°ä¸“æœ‰åè¯â‡¢æ–°ä¸“æœ‰åè¯
        ```
    """)
    
    try:
        # è°ƒç”¨LLM
        logging.info(f"ğŸ¤– è°ƒç”¨APIç¿»è¯‘æ‰¹æ¬¡ {batch_num}...")
        llm_output = call_llm(system_prompt, tagged_content, config)
        
        if not llm_output or not llm_output.strip():
            raise ValueError("LLMè¿”å›å†…å®¹ä¸ºç©º")
        
        # æ¸…æ´—è¾“å‡ºå¹¶å»é™¤æ ‡ç­¾
        cn_body, new_terms_block, miss_list = strip_tags(llm_output, keep_missing=True)
        
        # éªŒè¯ç¿»è¯‘è´¨é‡
        if not cn_body.strip():
            raise ValueError("ç¿»è¯‘ç»“æœä¸ºç©º")
        
        # æ£€æŸ¥æ®µè½æ•°é‡æ˜¯å¦åˆç†
        translated_segments = len(re.findall(r'<c\d+>', llm_output))
        logging.info(f"ğŸ“ ç¿»è¯‘å®Œæˆ: {original_segments} æ®µ â†’ {translated_segments} æ®µ")
        
        if abs(original_segments - translated_segments) > original_segments * 0.2:  # å…è®¸20%çš„å·®å¼‚
            warning_msg = f"åŸæ–‡{original_segments}æ®µ vs è¯‘æ–‡{translated_segments}æ®µ"
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æ®µè½æ•°é‡å·®å¼‚è¾ƒå¤§: {warning_msg}")
        
        # æ›´æ–°æœ¯è¯­è¡¨
        if new_terms_block:
            new_terms_count = 0
            for line in new_terms_block.splitlines():
                if "\t" in line or "â‡¢" in line:
                    # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šåˆ¶è¡¨ç¬¦åˆ†éš”æˆ–ç®­å¤´åˆ†éš”
                    if "â‡¢" in line:
                        src, tgt = [x.strip() for x in line.split("â‡¢", 1)]
                    else:
                        src, tgt = [x.strip() for x in line.split("\t", 1)]
                    
                    if src and tgt and src not in glossary:
                        glossary[src] = tgt
                        new_terms_count += 1
            
            if new_terms_count > 0:
                logging.info(f"ğŸ“š æ‰¹æ¬¡{batch_num}æ–°å¢{new_terms_count}ä¸ªæœ¯è¯­")
                # ä¿å­˜æ›´æ–°çš„æœ¯è¯­è¡¨
                glossary_path = output_path / "glossary.tsv"
                with open(glossary_path, 'w', encoding='utf-8') as f:
                    for k, v in glossary.items():
                        f.write(f"{k}\t{v}\n")
        
        # å¤‡ä»½å¹¶ä¿å­˜æ¸…æ´—åçš„å†…å®¹
        output_file = chap_dir / f"batch_{batch_num:03d}.md"
        if output_file.exists():
            backup_file = chap_dir / f"batch_{batch_num:03d}.md.backup"
            output_file.rename(backup_file)
            logging.info(f"ğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½")
        
        output_file.write_text(cn_body, encoding='utf-8')
        
        if miss_list:
            logging.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}æœ‰{len(miss_list)}ä¸ªç¼ºå¤±æ®µè½: {', '.join(miss_list)}")
        
        logging.info(f"âœ… æ‰¹æ¬¡ {batch_num} é‡æ–°ç¿»è¯‘å®Œæˆ")
        
        return True
        
    except Exception as e:
        logging.error(f"âŒ æ‰¹æ¬¡ {batch_num} ç¿»è¯‘å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='é‡æ–°ç¿»è¯‘æŒ‡å®šæ‰¹æ¬¡')
    parser.add_argument('--auto', action='store_true', help='è‡ªåŠ¨æ¨¡å¼ï¼Œè·³è¿‡äº¤äº’å¼æç¤º')
    
    args = parser.parse_args()
    
    # äº¤äº’å¼è·å–è¾“å‡ºç›®å½•
    output_dir = None
    if not args.auto:
        print("ğŸ”„ é‡æ–°ç¿»è¯‘å·¥å…·")
        print("=" * 30)
        
        while True:
            output_input = input("è¯·è¾“å…¥ä¹¦ç±ç›®å½•è·¯å¾„ (ä¾‹å¦‚: output/book1): ").strip()
            if output_input:
                output_dir = Path(output_input)
                if output_dir.exists() and output_dir.is_dir():
                    break
                else:
                    print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {output_input}")
                    continue
            else:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„ç›®å½•è·¯å¾„")
                continue
    else:
        print("âŒ è‡ªåŠ¨æ¨¡å¼éœ€è¦æŒ‡å®šä¹¦ç±ç›®å½•è·¯å¾„")
        return
    
    # ä»è¾“å‡ºç›®å½•åŠ è½½é…ç½®
    config_path = output_dir / "config.json"
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    config = load_config(str(config_path))
    
    # äº¤äº’å¼è·å–æ‰¹æ¬¡
    batches = []
    all_diff = False
    
    if not args.auto:
        print("\nè¯·é€‰æ‹©æ“ä½œæ–¹å¼:")
        print("1. é‡æ–°ç¿»è¯‘æŒ‡å®šæ‰¹æ¬¡")
        print("2. é‡æ–°ç¿»è¯‘æ‰€æœ‰å·®å¼‚è¾ƒå¤§çš„æ‰¹æ¬¡")
        print("3. æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        print("4. é€€å‡º")
        
        while True:
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4): ").strip()
            
            if choice == '1':
                batch_input = input("è¯·è¾“å…¥è¦é‡æ–°ç¿»è¯‘çš„æ‰¹æ¬¡å· (ç”¨ç©ºæ ¼åˆ†éš”å¤šä¸ªæ‰¹æ¬¡): ").strip()
                if batch_input:
                    try:
                        batches = [int(x) for x in batch_input.split()]
                        break
                    except ValueError:
                        print("âŒ æ‰¹æ¬¡å·å¿…é¡»æ˜¯æ•°å­—ï¼Œè¯·é‡æ–°è¾“å…¥")
                        continue
                else:
                    print("âŒ è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæ‰¹æ¬¡å·")
                    continue
            elif choice == '2':
                all_diff = True
                break
            elif choice == '3':
                parser.print_help()
                return
            elif choice == '4':
                print("ğŸ‘‹ å·²é€€å‡º")
                return
            else:
                print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·è¾“å…¥ 1-4")
                continue
    
    # åŠ è½½æœ¯è¯­è¡¨
    glossary_path = output_dir / "glossary.tsv"
    glossary = load_glossary(glossary_path)
    
    # ç¡®å®šè¦å¤„ç†çš„æ‰¹æ¬¡
    if all_diff:
        batches = find_diff_batches(output_dir)
        if not batches:
            print("âœ… æœªå‘ç°å·®å¼‚è¾ƒå¤§çš„æ‰¹æ¬¡")
            return
        print(f"ğŸ” å‘ç° {len(batches)} ä¸ªå·®å¼‚è¾ƒå¤§çš„æ‰¹æ¬¡: {batches}")
    
    # é‡æ–°ç¿»è¯‘
    success_count = 0
    for i, batch_num in enumerate(batches, 1):
        print(f"\n{'='*10} å¤„ç†æ‰¹æ¬¡ {batch_num} ({i}/{len(batches)}) {'='*10}")
        
        if retranslate_batch(batch_num, config, str(output_dir), glossary):
            success_count += 1
        
        # APIè°ƒç”¨é—´éš”
        if i < len(batches):
            time.sleep(2)
    
    # ç»“æœç»Ÿè®¡
    print(f"\nğŸ‰ å¤„ç†å®Œæˆ: {success_count}/{len(batches)} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
    
    # å¦‚æœæœ‰æˆåŠŸçš„é‡æ–°ç¿»è¯‘ï¼Œè‡ªåŠ¨åˆå¹¶markdownæ–‡ä»¶
    if success_count > 0:
        print(f"\nğŸ“š æ­£åœ¨é‡æ–°ç”Ÿæˆæ•´ä¹¦markdownæ–‡ä»¶...")
        if merge_markdown(config, output_dir):
            print(f"ğŸŠ æ•´ä¹¦markdownæ–‡ä»¶å·²æ›´æ–°!")
        else:
            print(f"âš ï¸  æ•´ä¹¦markdownæ–‡ä»¶æ›´æ–°å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ merge_md.py")

if __name__ == "__main__":
    main()